{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: biopython in /home/ubuntu/anaconda3/lib/python3.7/site-packages (1.81)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from biopython) (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/ubuntu/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (46.4.0.post20200518)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.34.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/ubuntu/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_short_reads(fasta_file, output_file, read_length=33):\n",
    "    short_reads = []\n",
    "    read_ids = []\n",
    "    types = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequence = str(record.seq)\n",
    "        arg_type = '_'.join(record.id.split('_')[1:])\n",
    "        # Generate short reads\n",
    "        for i in range(0, len(sequence), read_length):\n",
    "            if i + read_length <= len(sequence):  # Ensure we don't exceed the sequence length\n",
    "                short_read = Seq(sequence[i:i + read_length])\n",
    "                read_id = f\"{record.id}_pos_{i}\"\n",
    "                short_reads.append(SeqRecord(short_read, id=read_id, description=\"\"))\n",
    "                read_ids.append(read_id)\n",
    "                types.append(arg_type)\n",
    "\n",
    "    # Write the short reads to a new fasta file\n",
    "    SeqIO.write(short_reads, output_file, \"fasta\")\n",
    "    return short_reads,read_ids,types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fasta = \"uniprot_sequences.fasta\"\n",
    "output_fasta = \"short_reads.fasta\"\n",
    "short_reads,read_ids,types = split_to_short_reads(input_fasta, output_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_to_dataframe(fasta_file):\n",
    "    records = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        records.append({\"id\": record.id.split('|')[0]\n",
    "                        , \"db\": record.id.split('|')[2]\n",
    "                        , \"type\": record.id.split('|')[3]\n",
    "                        , \"sequence\": str(record.seq)})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "data = fasta_to_dataframe(\"features.fasta\")\n",
    "\n",
    "uniprot_data = data[data['db'] == 'UNIPROT']\n",
    "card_ardb_data = data[data['db'].isin(['CARD', 'ARDB'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_matrix(scoring_path, read_ids, reference_df):\n",
    "    # Extract all unique query and target IDs\n",
    "    query_ids = read_ids\n",
    "    target_ids = [f\"{row.id}_{row.type}\" for _, row in reference_df.iterrows()]\n",
    "\n",
    "    # Initialize a dictionary to hold scores with 0s for all query-target pairs\n",
    "    scores_dict = {query_id: {target_id: 0.0 for target_id in target_ids} for query_id in query_ids}\n",
    "\n",
    "    # Populate the scores_dict with bit scores from DIAMOND output\n",
    "    with open(scoring_path) as f:\n",
    "        for line in f:\n",
    "            query_id, subject_id, identity, alignment_length, mismatches, gap_opens, q_start, q_end, s_start, s_end, evalue, bit_score = line.strip().split()\n",
    "            if query_id in scores_dict and subject_id in scores_dict[query_id]:\n",
    "                scores_dict[query_id][subject_id] = float(bit_score)\n",
    "    \n",
    "    # Convert scores_dict to a feature matrix\n",
    "    all_scores = []\n",
    "    for query_id in query_ids:\n",
    "        # Each row is a list of bit scores for the current query against each target in the reference\n",
    "        scores = [scores_dict[query_id][target_id] for target_id in target_ids]\n",
    "        all_scores.append(scores)\n",
    "    \n",
    "    # Convert to numpy array for matrix operations\n",
    "    feature_matrix = np.array(all_scores)\n",
    "\n",
    "    # Normalize the matrix\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_features = scaler.fit_transform(feature_matrix)\n",
    "    \n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = generate_feature_matrix('out_sr.tsv', read_ids, card_ardb_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99253, 4355)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feature_matrix.pkl', 'rb') as handle:\n",
    "    feature_matrix = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, types, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69477, 4355)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6947"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62530"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train) \n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARGMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DeepARGMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 2000)\n",
    "        self.fc2 = nn.Linear(2000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 500)\n",
    "        self.fc4 = nn.Linear(500, 100)\n",
    "        self.output = nn.Linear(100, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return torch.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "num_classes = uniprot_data['type'].nunique()\n",
    "\n",
    "model = DeepARGMLP(input_dim=input_dim, output_dim=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3959, Val Loss: 2.2565, Val Accuracy: 55.72%\n",
      "Epoch 2/10, Loss: 2.2495, Val Loss: 2.2491, Val Accuracy: 56.24%\n",
      "Epoch 3/10, Loss: 2.2462, Val Loss: 2.2434, Val Accuracy: 56.28%\n",
      "Epoch 4/10, Loss: 2.1715, Val Loss: 2.1458, Val Accuracy: 66.57%\n",
      "Epoch 5/10, Loss: 2.1039, Val Loss: 2.0815, Val Accuracy: 73.43%\n",
      "Epoch 6/10, Loss: 2.0819, Val Loss: 2.0811, Val Accuracy: 73.41%\n",
      "Epoch 7/10, Loss: 2.0800, Val Loss: 2.0805, Val Accuracy: 73.46%\n",
      "Epoch 8/10, Loss: 2.0793, Val Loss: 2.0802, Val Accuracy: 73.48%\n",
      "Epoch 9/10, Loss: 2.0786, Val Loss: 2.0795, Val Accuracy: 73.50%\n",
      "Epoch 10/10, Loss: 2.0777, Val Loss: 2.0780, Val Accuracy: 73.50%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            outputs = model(X_val_batch)\n",
    "            loss = criterion(outputs, y_val_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_val_batch.size(0)\n",
    "            correct += (predicted == y_val_batch).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "          f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1\n",
      "Fold 1 Epoch 1/100, Loss: 2.5345, Val Loss: 2.3232, F1 Score: 0.3994\n",
      "Fold 1 Epoch 2/100, Loss: 2.2704, Val Loss: 2.2474, F1 Score: 0.4544\n",
      "Fold 1 Epoch 3/100, Loss: 2.2503, Val Loss: 2.2436, F1 Score: 0.4552\n",
      "Fold 1 Epoch 4/100, Loss: 2.2477, Val Loss: 2.2423, F1 Score: 0.4478\n",
      "Fold 1 Epoch 5/100, Loss: 2.2479, Val Loss: 2.2421, F1 Score: 0.4473\n",
      "Fold 1 Epoch 6/100, Loss: 2.2475, Val Loss: 2.2425, F1 Score: 0.4634\n",
      "Fold 1 Epoch 7/100, Loss: 2.2460, Val Loss: 2.2418, F1 Score: 0.4572\n",
      "Fold 1 Epoch 8/100, Loss: 2.2451, Val Loss: 2.2400, F1 Score: 0.4519\n",
      "Fold 1 Epoch 9/100, Loss: 2.2293, Val Loss: 2.1955, F1 Score: 0.5978\n",
      "Fold 1 Epoch 10/100, Loss: 2.1588, Val Loss: 2.1425, F1 Score: 0.5967\n",
      "Fold 1 Epoch 11/100, Loss: 2.1476, Val Loss: 2.1422, F1 Score: 0.6000\n",
      "Fold 1 Epoch 12/100, Loss: 2.1436, Val Loss: 2.1307, F1 Score: 0.5959\n",
      "Fold 1 Epoch 13/100, Loss: 2.1042, Val Loss: 2.0826, F1 Score: 0.7008\n",
      "Fold 1 Epoch 14/100, Loss: 2.0840, Val Loss: 2.0787, F1 Score: 0.7015\n",
      "Fold 1 Epoch 15/100, Loss: 2.0806, Val Loss: 2.0783, F1 Score: 0.7049\n",
      "Fold 1 Epoch 16/100, Loss: 2.0800, Val Loss: 2.0776, F1 Score: 0.7040\n",
      "Fold 1 Epoch 17/100, Loss: 2.0793, Val Loss: 2.0767, F1 Score: 0.7016\n",
      "Fold 1 Epoch 18/100, Loss: 2.0789, Val Loss: 2.0765, F1 Score: 0.7018\n",
      "Fold 1 Epoch 19/100, Loss: 2.0797, Val Loss: 2.0764, F1 Score: 0.7023\n",
      "Fold 1 Epoch 20/100, Loss: 2.0785, Val Loss: 2.0764, F1 Score: 0.7048\n",
      "Fold 1 Epoch 21/100, Loss: 2.0783, Val Loss: 2.0764, F1 Score: 0.7028\n",
      "Fold 1 Epoch 22/100, Loss: 2.0784, Val Loss: 2.0764, F1 Score: 0.7015\n",
      "Fold 1 Epoch 23/100, Loss: 2.0792, Val Loss: 2.0763, F1 Score: 0.7012\n",
      "Fold 1 Epoch 24/100, Loss: 2.0780, Val Loss: 2.0762, F1 Score: 0.7022\n",
      "Fold 1 Epoch 25/100, Loss: 2.0779, Val Loss: 2.0761, F1 Score: 0.7026\n",
      "Fold 1 Epoch 26/100, Loss: 2.0788, Val Loss: 2.0760, F1 Score: 0.7032\n",
      "Fold 1 Epoch 27/100, Loss: 2.0775, Val Loss: 2.0756, F1 Score: 0.7042\n",
      "Fold 1 Epoch 28/100, Loss: 2.0772, Val Loss: 2.0750, F1 Score: 0.7047\n",
      "Fold 1 Epoch 29/100, Loss: 2.0763, Val Loss: 2.0730, F1 Score: 0.7059\n",
      "Fold 1 Epoch 30/100, Loss: 2.0702, Val Loss: 2.0556, F1 Score: 0.7456\n",
      "Fold 1 Epoch 31/100, Loss: 2.0588, Val Loss: 2.0527, F1 Score: 0.7446\n",
      "Fold 1 Epoch 32/100, Loss: 2.0562, Val Loss: 2.0519, F1 Score: 0.7445\n",
      "Fold 1 Epoch 33/100, Loss: 2.0561, Val Loss: 2.0518, F1 Score: 0.7453\n",
      "Fold 1 Epoch 34/100, Loss: 2.0542, Val Loss: 2.0504, F1 Score: 0.7460\n",
      "Fold 1 Epoch 35/100, Loss: 2.0519, Val Loss: 2.0450, F1 Score: 0.7626\n",
      "Fold 1 Epoch 36/100, Loss: 2.0481, Val Loss: 2.0404, F1 Score: 0.7661\n",
      "Fold 1 Epoch 37/100, Loss: 2.0442, Val Loss: 2.0404, F1 Score: 0.7646\n",
      "Fold 1 Epoch 38/100, Loss: 2.0425, Val Loss: 2.0393, F1 Score: 0.7652\n",
      "Fold 1 Epoch 39/100, Loss: 2.0410, Val Loss: 2.0369, F1 Score: 0.7659\n",
      "Fold 1 Epoch 40/100, Loss: 2.0386, Val Loss: 2.0310, F1 Score: 0.7828\n",
      "Fold 1 Epoch 41/100, Loss: 2.0341, Val Loss: 2.0253, F1 Score: 0.7885\n",
      "Fold 1 Epoch 42/100, Loss: 2.0299, Val Loss: 2.0240, F1 Score: 0.7881\n",
      "Fold 1 Epoch 43/100, Loss: 2.0276, Val Loss: 2.0232, F1 Score: 0.7887\n",
      "Fold 1 Epoch 44/100, Loss: 2.0269, Val Loss: 2.0232, F1 Score: 0.7889\n",
      "Fold 1 Epoch 45/100, Loss: 2.0264, Val Loss: 2.0227, F1 Score: 0.7889\n",
      "Fold 1 Epoch 46/100, Loss: 2.0258, Val Loss: 2.0229, F1 Score: 0.7888\n",
      "Fold 1 Epoch 47/100, Loss: 2.0264, Val Loss: 2.0226, F1 Score: 0.7892\n",
      "Fold 1 Epoch 48/100, Loss: 2.0247, Val Loss: 2.0224, F1 Score: 0.7890\n",
      "Fold 1 Epoch 49/100, Loss: 2.0250, Val Loss: 2.0224, F1 Score: 0.7890\n",
      "Fold 1 Epoch 50/100, Loss: 2.0249, Val Loss: 2.0225, F1 Score: 0.7892\n",
      "Fold 1 Epoch 51/100, Loss: 2.0246, Val Loss: 2.0222, F1 Score: 0.7892\n",
      "Fold 1 Epoch 52/100, Loss: 2.0256, Val Loss: 2.0223, F1 Score: 0.7892\n",
      "Fold 1 Epoch 53/100, Loss: 2.0244, Val Loss: 2.0224, F1 Score: 0.7893\n",
      "Fold 1 Epoch 54/100, Loss: 2.0244, Val Loss: 2.0222, F1 Score: 0.7895\n",
      "Fold 1 Epoch 55/100, Loss: 2.0243, Val Loss: 2.0221, F1 Score: 0.7896\n",
      "Fold 1 Epoch 56/100, Loss: 2.0243, Val Loss: 2.0222, F1 Score: 0.7897\n",
      "Fold 1 Epoch 57/100, Loss: 2.0244, Val Loss: 2.0221, F1 Score: 0.7896\n",
      "Fold 1 Epoch 58/100, Loss: 2.0242, Val Loss: 2.0219, F1 Score: 0.7899\n",
      "Fold 1 Epoch 59/100, Loss: 2.0251, Val Loss: 2.0221, F1 Score: 0.7899\n",
      "Fold 1 Epoch 60/100, Loss: 2.0240, Val Loss: 2.0220, F1 Score: 0.7895\n",
      "Fold 1 Epoch 61/100, Loss: 2.0240, Val Loss: 2.0219, F1 Score: 0.7897\n",
      "Fold 1 Epoch 62/100, Loss: 2.0239, Val Loss: 2.0220, F1 Score: 0.7896\n",
      "Fold 1 Epoch 63/100, Loss: 2.0239, Val Loss: 2.0221, F1 Score: 0.7896\n",
      "Fold 1 Epoch 64/100, Loss: 2.0238, Val Loss: 2.0221, F1 Score: 0.7896\n",
      "Fold 1 Epoch 65/100, Loss: 2.0239, Val Loss: 2.0221, F1 Score: 0.7896\n",
      "Fold 1 Epoch 66/100, Loss: 2.0250, Val Loss: 2.0221, F1 Score: 0.7897\n",
      "Fold 1 Epoch 67/100, Loss: 2.0238, Val Loss: 2.0221, F1 Score: 0.7895\n",
      "Fold 1 Epoch 68/100, Loss: 2.0240, Val Loss: 2.0221, F1 Score: 0.7895\n",
      "Fold 1 Epoch 69/100, Loss: 2.0239, Val Loss: 2.0221, F1 Score: 0.7896\n",
      "Fold 1 Epoch 70/100, Loss: 2.0238, Val Loss: 2.0220, F1 Score: 0.7896\n",
      "Fold 1 Epoch 71/100, Loss: 2.0239, Val Loss: 2.0221, F1 Score: 0.7896\n",
      "Fold 1 Epoch 72/100, Loss: 2.0238, Val Loss: 2.0221, F1 Score: 0.7895\n",
      "Fold 1 Epoch 73/100, Loss: 2.0238, Val Loss: 2.0220, F1 Score: 0.7896\n",
      "Fold 1 Epoch 74/100, Loss: 2.0247, Val Loss: 2.0220, F1 Score: 0.7895\n",
      "Fold 1 Epoch 75/100, Loss: 2.0237, Val Loss: 2.0221, F1 Score: 0.7896\n",
      "Fold 1 Epoch 76/100, Loss: 2.0238, Val Loss: 2.0220, F1 Score: 0.7897\n",
      "Fold 1 Epoch 77/100, Loss: 2.0246, Val Loss: 2.0220, F1 Score: 0.7896\n",
      "Fold 1 Epoch 78/100, Loss: 2.0237, Val Loss: 2.0221, F1 Score: 0.7899\n",
      "Fold 1 Epoch 79/100, Loss: 2.0237, Val Loss: 2.0220, F1 Score: 0.7897\n",
      "Fold 1 Epoch 80/100, Loss: 2.0236, Val Loss: 2.0220, F1 Score: 0.7897\n",
      "Fold 1 Epoch 81/100, Loss: 2.0237, Val Loss: 2.0221, F1 Score: 0.7898\n",
      "Fold 1 Epoch 82/100, Loss: 2.0246, Val Loss: 2.0220, F1 Score: 0.7896\n",
      "Fold 1 Epoch 83/100, Loss: 2.0236, Val Loss: 2.0221, F1 Score: 0.7898\n",
      "Fold 1 Epoch 84/100, Loss: 2.0246, Val Loss: 2.0220, F1 Score: 0.7897\n",
      "Fold 1 Epoch 85/100, Loss: 2.0246, Val Loss: 2.0221, F1 Score: 0.7899\n",
      "Fold 1 Epoch 86/100, Loss: 2.0236, Val Loss: 2.0221, F1 Score: 0.7899\n",
      "Fold 1 Epoch 87/100, Loss: 2.0236, Val Loss: 2.0220, F1 Score: 0.7897\n",
      "Fold 1 Epoch 88/100, Loss: 2.0236, Val Loss: 2.0220, F1 Score: 0.7898\n",
      "Fold 1 Epoch 89/100, Loss: 2.0235, Val Loss: 2.0220, F1 Score: 0.7898\n",
      "Fold 1 Epoch 90/100, Loss: 2.0236, Val Loss: 2.0220, F1 Score: 0.7898\n",
      "Fold 1 Epoch 91/100, Loss: 2.0235, Val Loss: 2.0220, F1 Score: 0.7897\n",
      "Fold 1 Epoch 92/100, Loss: 2.0246, Val Loss: 2.0220, F1 Score: 0.7897\n",
      "Fold 1 Epoch 93/100, Loss: 2.0235, Val Loss: 2.0220, F1 Score: 0.7898\n",
      "Fold 1 Epoch 94/100, Loss: 2.0236, Val Loss: 2.0220, F1 Score: 0.7898\n",
      "Fold 1 Epoch 95/100, Loss: 2.0245, Val Loss: 2.0219, F1 Score: 0.7896\n",
      "Fold 1 Epoch 96/100, Loss: 2.0235, Val Loss: 2.0219, F1 Score: 0.7897\n",
      "Fold 1 Epoch 97/100, Loss: 2.0235, Val Loss: 2.0219, F1 Score: 0.7897\n",
      "Fold 1 Epoch 98/100, Loss: 2.0235, Val Loss: 2.0219, F1 Score: 0.7898\n",
      "Fold 1 Epoch 99/100, Loss: 2.0235, Val Loss: 2.0219, F1 Score: 0.7897\n",
      "Fold 1 Epoch 100/100, Loss: 2.0235, Val Loss: 2.0219, F1 Score: 0.7897\n",
      "Starting Fold 2\n",
      "Fold 2 Epoch 1/100, Loss: 2.5255, Val Loss: 2.3259, F1 Score: 0.4152\n",
      "Fold 2 Epoch 2/100, Loss: 2.2706, Val Loss: 2.2515, F1 Score: 0.4585\n",
      "Fold 2 Epoch 3/100, Loss: 2.2504, Val Loss: 2.2487, F1 Score: 0.4502\n",
      "Fold 2 Epoch 4/100, Loss: 2.2473, Val Loss: 2.2479, F1 Score: 0.4540\n",
      "Fold 2 Epoch 5/100, Loss: 2.2475, Val Loss: 2.2465, F1 Score: 0.4502\n",
      "Fold 2 Epoch 6/100, Loss: 2.2459, Val Loss: 2.2464, F1 Score: 0.4526\n",
      "Fold 2 Epoch 7/100, Loss: 2.2453, Val Loss: 2.2455, F1 Score: 0.4516\n",
      "Fold 2 Epoch 8/100, Loss: 2.2435, Val Loss: 2.2336, F1 Score: 0.4397\n",
      "Fold 2 Epoch 9/100, Loss: 2.1794, Val Loss: 2.1495, F1 Score: 0.5899\n",
      "Fold 2 Epoch 10/100, Loss: 2.1448, Val Loss: 2.1308, F1 Score: 0.5995\n",
      "Fold 2 Epoch 11/100, Loss: 2.1006, Val Loss: 2.0864, F1 Score: 0.6971\n",
      "Fold 2 Epoch 12/100, Loss: 2.0830, Val Loss: 2.0827, F1 Score: 0.6977\n",
      "Fold 2 Epoch 13/100, Loss: 2.0805, Val Loss: 2.0819, F1 Score: 0.6985\n",
      "Fold 2 Epoch 14/100, Loss: 2.0804, Val Loss: 2.0816, F1 Score: 0.6992\n",
      "Fold 2 Epoch 15/100, Loss: 2.0785, Val Loss: 2.0810, F1 Score: 0.6997\n",
      "Fold 2 Epoch 16/100, Loss: 2.0779, Val Loss: 2.0806, F1 Score: 0.7003\n",
      "Fold 2 Epoch 17/100, Loss: 2.0774, Val Loss: 2.0795, F1 Score: 0.7013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18/100, Loss: 2.0760, Val Loss: 2.0766, F1 Score: 0.7028\n",
      "Fold 2 Epoch 19/100, Loss: 2.0675, Val Loss: 2.0608, F1 Score: 0.7406\n",
      "Fold 2 Epoch 20/100, Loss: 2.0592, Val Loss: 2.0592, F1 Score: 0.7395\n",
      "Fold 2 Epoch 21/100, Loss: 2.0571, Val Loss: 2.0584, F1 Score: 0.7395\n",
      "Fold 2 Epoch 22/100, Loss: 2.0550, Val Loss: 2.0571, F1 Score: 0.7398\n",
      "Fold 2 Epoch 23/100, Loss: 2.0520, Val Loss: 2.0478, F1 Score: 0.7618\n",
      "Fold 2 Epoch 24/100, Loss: 2.0460, Val Loss: 2.0454, F1 Score: 0.7602\n",
      "Fold 2 Epoch 25/100, Loss: 2.0431, Val Loss: 2.0448, F1 Score: 0.7601\n",
      "Fold 2 Epoch 26/100, Loss: 2.0423, Val Loss: 2.0446, F1 Score: 0.7604\n",
      "Fold 2 Epoch 27/100, Loss: 2.0406, Val Loss: 2.0443, F1 Score: 0.7604\n",
      "Fold 2 Epoch 28/100, Loss: 2.0403, Val Loss: 2.0442, F1 Score: 0.7600\n",
      "Fold 2 Epoch 29/100, Loss: 2.0400, Val Loss: 2.0442, F1 Score: 0.7600\n",
      "Fold 2 Epoch 30/100, Loss: 2.0408, Val Loss: 2.0442, F1 Score: 0.7601\n",
      "Fold 2 Epoch 31/100, Loss: 2.0395, Val Loss: 2.0442, F1 Score: 0.7601\n",
      "Fold 2 Epoch 32/100, Loss: 2.0395, Val Loss: 2.0442, F1 Score: 0.7602\n",
      "Fold 2 Epoch 33/100, Loss: 2.0404, Val Loss: 2.0441, F1 Score: 0.7602\n",
      "Fold 2 Epoch 34/100, Loss: 2.0392, Val Loss: 2.0441, F1 Score: 0.7602\n",
      "Fold 2 Epoch 35/100, Loss: 2.0390, Val Loss: 2.0441, F1 Score: 0.7604\n",
      "Fold 2 Epoch 36/100, Loss: 2.0389, Val Loss: 2.0440, F1 Score: 0.7604\n",
      "Fold 2 Epoch 37/100, Loss: 2.0398, Val Loss: 2.0441, F1 Score: 0.7605\n",
      "Fold 2 Epoch 38/100, Loss: 2.0388, Val Loss: 2.0439, F1 Score: 0.7602\n",
      "Fold 2 Epoch 39/100, Loss: 2.0399, Val Loss: 2.0440, F1 Score: 0.7611\n",
      "Fold 2 Epoch 40/100, Loss: 2.0384, Val Loss: 2.0439, F1 Score: 0.7605\n",
      "Fold 2 Epoch 41/100, Loss: 2.0383, Val Loss: 2.0431, F1 Score: 0.7607\n",
      "Fold 2 Epoch 42/100, Loss: 2.0346, Val Loss: 2.0328, F1 Score: 0.7819\n",
      "Fold 2 Epoch 43/100, Loss: 2.0291, Val Loss: 2.0310, F1 Score: 0.7825\n",
      "Fold 2 Epoch 44/100, Loss: 2.0273, Val Loss: 2.0306, F1 Score: 0.7826\n",
      "Fold 2 Epoch 45/100, Loss: 2.0259, Val Loss: 2.0304, F1 Score: 0.7824\n",
      "Fold 2 Epoch 46/100, Loss: 2.0257, Val Loss: 2.0301, F1 Score: 0.7824\n",
      "Fold 2 Epoch 47/100, Loss: 2.0251, Val Loss: 2.0298, F1 Score: 0.7830\n",
      "Fold 2 Epoch 48/100, Loss: 2.0256, Val Loss: 2.0296, F1 Score: 0.7830\n",
      "Fold 2 Epoch 49/100, Loss: 2.0243, Val Loss: 2.0295, F1 Score: 0.7837\n",
      "Fold 2 Epoch 50/100, Loss: 2.0241, Val Loss: 2.0293, F1 Score: 0.7835\n",
      "Fold 2 Epoch 51/100, Loss: 2.0242, Val Loss: 2.0294, F1 Score: 0.7835\n",
      "Fold 2 Epoch 52/100, Loss: 2.0240, Val Loss: 2.0294, F1 Score: 0.7833\n",
      "Fold 2 Epoch 53/100, Loss: 2.0237, Val Loss: 2.0295, F1 Score: 0.7834\n",
      "Fold 2 Epoch 54/100, Loss: 2.0237, Val Loss: 2.0293, F1 Score: 0.7835\n",
      "Fold 2 Epoch 55/100, Loss: 2.0235, Val Loss: 2.0293, F1 Score: 0.7835\n",
      "Fold 2 Epoch 56/100, Loss: 2.0234, Val Loss: 2.0293, F1 Score: 0.7838\n",
      "Fold 2 Epoch 57/100, Loss: 2.0236, Val Loss: 2.0293, F1 Score: 0.7834\n",
      "Fold 2 Epoch 58/100, Loss: 2.0245, Val Loss: 2.0293, F1 Score: 0.7834\n",
      "Fold 2 Epoch 59/100, Loss: 2.0234, Val Loss: 2.0292, F1 Score: 0.7832\n",
      "Fold 2 Epoch 60/100, Loss: 2.0245, Val Loss: 2.0292, F1 Score: 0.7836\n",
      "Fold 2 Epoch 61/100, Loss: 2.0244, Val Loss: 2.0292, F1 Score: 0.7839\n",
      "Fold 2 Epoch 62/100, Loss: 2.0233, Val Loss: 2.0292, F1 Score: 0.7837\n",
      "Fold 2 Epoch 63/100, Loss: 2.0233, Val Loss: 2.0292, F1 Score: 0.7836\n",
      "Fold 2 Epoch 64/100, Loss: 2.0243, Val Loss: 2.0292, F1 Score: 0.7833\n",
      "Fold 2 Epoch 65/100, Loss: 2.0232, Val Loss: 2.0292, F1 Score: 0.7832\n",
      "Fold 2 Epoch 66/100, Loss: 2.0231, Val Loss: 2.0292, F1 Score: 0.7837\n",
      "Fold 2 Epoch 67/100, Loss: 2.0233, Val Loss: 2.0291, F1 Score: 0.7839\n",
      "Fold 2 Epoch 68/100, Loss: 2.0231, Val Loss: 2.0291, F1 Score: 0.7835\n",
      "Fold 2 Epoch 69/100, Loss: 2.0231, Val Loss: 2.0292, F1 Score: 0.7833\n",
      "Fold 2 Epoch 70/100, Loss: 2.0242, Val Loss: 2.0291, F1 Score: 0.7836\n",
      "Fold 2 Epoch 71/100, Loss: 2.0232, Val Loss: 2.0292, F1 Score: 0.7834\n",
      "Fold 2 Epoch 72/100, Loss: 2.0230, Val Loss: 2.0292, F1 Score: 0.7834\n",
      "Fold 2 Epoch 73/100, Loss: 2.0231, Val Loss: 2.0292, F1 Score: 0.7833\n",
      "Fold 2 Epoch 74/100, Loss: 2.0231, Val Loss: 2.0292, F1 Score: 0.7839\n",
      "Fold 2 Epoch 75/100, Loss: 2.0240, Val Loss: 2.0291, F1 Score: 0.7833\n",
      "Fold 2 Epoch 76/100, Loss: 2.0229, Val Loss: 2.0291, F1 Score: 0.7833\n",
      "Fold 2 Epoch 77/100, Loss: 2.0231, Val Loss: 2.0291, F1 Score: 0.7833\n",
      "Fold 2 Epoch 78/100, Loss: 2.0231, Val Loss: 2.0291, F1 Score: 0.7835\n",
      "Fold 2 Epoch 79/100, Loss: 2.0232, Val Loss: 2.0291, F1 Score: 0.7834\n",
      "Fold 2 Epoch 80/100, Loss: 2.0240, Val Loss: 2.0292, F1 Score: 0.7836\n",
      "Fold 2 Epoch 81/100, Loss: 2.0231, Val Loss: 2.0291, F1 Score: 0.7841\n",
      "Fold 2 Epoch 82/100, Loss: 2.0230, Val Loss: 2.0292, F1 Score: 0.7836\n",
      "Fold 2 Epoch 83/100, Loss: 2.0239, Val Loss: 2.0291, F1 Score: 0.7832\n",
      "Fold 2 Epoch 84/100, Loss: 2.0229, Val Loss: 2.0291, F1 Score: 0.7836\n",
      "Fold 2 Epoch 85/100, Loss: 2.0229, Val Loss: 2.0291, F1 Score: 0.7836\n",
      "Fold 2 Epoch 86/100, Loss: 2.0229, Val Loss: 2.0291, F1 Score: 0.7839\n",
      "Fold 2 Epoch 87/100, Loss: 2.0239, Val Loss: 2.0291, F1 Score: 0.7835\n",
      "Fold 2 Epoch 88/100, Loss: 2.0232, Val Loss: 2.0292, F1 Score: 0.7838\n",
      "Fold 2 Epoch 89/100, Loss: 2.0239, Val Loss: 2.0291, F1 Score: 0.7838\n",
      "Fold 2 Epoch 90/100, Loss: 2.0229, Val Loss: 2.0291, F1 Score: 0.7832\n",
      "Fold 2 Epoch 91/100, Loss: 2.0230, Val Loss: 2.0291, F1 Score: 0.7836\n",
      "Fold 2 Epoch 92/100, Loss: 2.0229, Val Loss: 2.0292, F1 Score: 0.7834\n",
      "Fold 2 Epoch 93/100, Loss: 2.0239, Val Loss: 2.0291, F1 Score: 0.7833\n",
      "Fold 2 Epoch 94/100, Loss: 2.0229, Val Loss: 2.0291, F1 Score: 0.7833\n",
      "Fold 2 Epoch 95/100, Loss: 2.0229, Val Loss: 2.0291, F1 Score: 0.7837\n",
      "Fold 2 Epoch 96/100, Loss: 2.0239, Val Loss: 2.0291, F1 Score: 0.7834\n",
      "Fold 2 Epoch 97/100, Loss: 2.0228, Val Loss: 2.0291, F1 Score: 0.7836\n",
      "Fold 2 Epoch 98/100, Loss: 2.0229, Val Loss: 2.0291, F1 Score: 0.7834\n",
      "Fold 2 Epoch 99/100, Loss: 2.0238, Val Loss: 2.0291, F1 Score: 0.7835\n",
      "Fold 2 Epoch 100/100, Loss: 2.0229, Val Loss: 2.0291, F1 Score: 0.7835\n",
      "Starting Fold 3\n",
      "Fold 3 Epoch 1/100, Loss: 2.5329, Val Loss: 2.3231, F1 Score: 0.4102\n",
      "Fold 3 Epoch 2/100, Loss: 2.2704, Val Loss: 2.2502, F1 Score: 0.4515\n",
      "Fold 3 Epoch 3/100, Loss: 2.2503, Val Loss: 2.2465, F1 Score: 0.4430\n",
      "Fold 3 Epoch 4/100, Loss: 2.2475, Val Loss: 2.2460, F1 Score: 0.4432\n",
      "Fold 3 Epoch 5/100, Loss: 2.2465, Val Loss: 2.2459, F1 Score: 0.4460\n",
      "Fold 3 Epoch 6/100, Loss: 2.2467, Val Loss: 2.2453, F1 Score: 0.4506\n",
      "Fold 3 Epoch 7/100, Loss: 2.2444, Val Loss: 2.2421, F1 Score: 0.4526\n",
      "Fold 3 Epoch 8/100, Loss: 2.2010, Val Loss: 2.1485, F1 Score: 0.5926\n",
      "Fold 3 Epoch 9/100, Loss: 2.1512, Val Loss: 2.1436, F1 Score: 0.5951\n",
      "Fold 3 Epoch 10/100, Loss: 2.1437, Val Loss: 2.1300, F1 Score: 0.5952\n",
      "Fold 3 Epoch 11/100, Loss: 2.1012, Val Loss: 2.0823, F1 Score: 0.7018\n",
      "Fold 3 Epoch 12/100, Loss: 2.0834, Val Loss: 2.0785, F1 Score: 0.7026\n",
      "Fold 3 Epoch 13/100, Loss: 2.0807, Val Loss: 2.0777, F1 Score: 0.7043\n",
      "Fold 3 Epoch 14/100, Loss: 2.0804, Val Loss: 2.2873, F1 Score: 0.4816\n",
      "Fold 3 Epoch 15/100, Loss: 2.2203, Val Loss: 2.0800, F1 Score: 0.7070\n",
      "Fold 3 Epoch 16/100, Loss: 2.0813, Val Loss: 2.0782, F1 Score: 0.7064\n",
      "Fold 3 Epoch 17/100, Loss: 2.0795, Val Loss: 2.0772, F1 Score: 0.7069\n",
      "Fold 3 Epoch 18/100, Loss: 2.0795, Val Loss: 2.0766, F1 Score: 0.7070\n",
      "Fold 3 Epoch 19/100, Loss: 2.0777, Val Loss: 2.0753, F1 Score: 0.7074\n",
      "Fold 3 Epoch 20/100, Loss: 2.0775, Val Loss: 2.0720, F1 Score: 0.7063\n",
      "Fold 3 Epoch 21/100, Loss: 2.0687, Val Loss: 2.0557, F1 Score: 0.7469\n",
      "Fold 3 Epoch 22/100, Loss: 2.0604, Val Loss: 2.0533, F1 Score: 0.7464\n",
      "Fold 3 Epoch 23/100, Loss: 2.0570, Val Loss: 2.0530, F1 Score: 0.7439\n",
      "Fold 3 Epoch 24/100, Loss: 2.0568, Val Loss: 2.0524, F1 Score: 0.7440\n",
      "Fold 3 Epoch 25/100, Loss: 2.0550, Val Loss: 2.0523, F1 Score: 0.7439\n",
      "Fold 3 Epoch 26/100, Loss: 2.0548, Val Loss: 2.0520, F1 Score: 0.7439\n",
      "Fold 3 Epoch 27/100, Loss: 2.0553, Val Loss: 2.0517, F1 Score: 0.7447\n",
      "Fold 3 Epoch 28/100, Loss: 2.0537, Val Loss: 2.0508, F1 Score: 0.7446\n",
      "Fold 3 Epoch 29/100, Loss: 2.0519, Val Loss: 2.0455, F1 Score: 0.7623\n",
      "Fold 3 Epoch 30/100, Loss: 2.0470, Val Loss: 2.0392, F1 Score: 0.7681\n",
      "Fold 3 Epoch 31/100, Loss: 2.0445, Val Loss: 2.0370, F1 Score: 0.7682\n",
      "Fold 3 Epoch 32/100, Loss: 2.0412, Val Loss: 2.0327, F1 Score: 0.7856\n",
      "Fold 3 Epoch 33/100, Loss: 2.0387, Val Loss: 2.0277, F1 Score: 0.7879\n",
      "Fold 3 Epoch 34/100, Loss: 2.0336, Val Loss: 2.0254, F1 Score: 0.7886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 35/100, Loss: 2.0314, Val Loss: 2.0242, F1 Score: 0.7890\n",
      "Fold 3 Epoch 36/100, Loss: 2.0294, Val Loss: 2.0235, F1 Score: 0.7894\n",
      "Fold 3 Epoch 37/100, Loss: 2.0287, Val Loss: 2.0234, F1 Score: 0.7900\n",
      "Fold 3 Epoch 38/100, Loss: 2.0276, Val Loss: 2.0231, F1 Score: 0.7895\n",
      "Fold 3 Epoch 39/100, Loss: 2.0272, Val Loss: 2.0230, F1 Score: 0.7895\n",
      "Fold 3 Epoch 40/100, Loss: 2.0268, Val Loss: 2.0230, F1 Score: 0.7894\n",
      "Fold 3 Epoch 41/100, Loss: 2.0261, Val Loss: 2.0229, F1 Score: 0.7898\n",
      "Fold 3 Epoch 42/100, Loss: 2.0264, Val Loss: 2.0231, F1 Score: 0.7895\n",
      "Fold 3 Epoch 43/100, Loss: 2.0268, Val Loss: 2.0230, F1 Score: 0.7896\n",
      "Fold 3 Epoch 44/100, Loss: 2.0263, Val Loss: 2.0229, F1 Score: 0.7894\n",
      "Fold 3 Epoch 45/100, Loss: 2.0253, Val Loss: 2.0233, F1 Score: 0.7896\n",
      "Fold 3 Epoch 46/100, Loss: 2.0250, Val Loss: 2.0228, F1 Score: 0.7898\n",
      "Fold 3 Epoch 47/100, Loss: 2.0260, Val Loss: 2.0228, F1 Score: 0.7896\n",
      "Fold 3 Epoch 48/100, Loss: 2.0248, Val Loss: 2.0229, F1 Score: 0.7898\n",
      "Fold 3 Epoch 49/100, Loss: 2.0249, Val Loss: 2.0230, F1 Score: 0.7896\n",
      "Fold 3 Epoch 50/100, Loss: 2.0249, Val Loss: 2.0227, F1 Score: 0.7897\n",
      "Fold 3 Epoch 51/100, Loss: 2.0246, Val Loss: 2.0228, F1 Score: 0.7896\n",
      "Fold 3 Epoch 52/100, Loss: 2.0245, Val Loss: 2.0229, F1 Score: 0.7896\n",
      "Fold 3 Epoch 53/100, Loss: 2.0247, Val Loss: 2.0228, F1 Score: 0.7897\n",
      "Fold 3 Epoch 54/100, Loss: 2.0245, Val Loss: 2.0228, F1 Score: 0.7896\n",
      "Fold 3 Epoch 55/100, Loss: 2.0245, Val Loss: 2.0228, F1 Score: 0.7896\n",
      "Fold 3 Epoch 56/100, Loss: 2.0244, Val Loss: 2.0229, F1 Score: 0.7899\n",
      "Fold 3 Epoch 57/100, Loss: 2.0244, Val Loss: 2.0227, F1 Score: 0.7898\n",
      "Fold 3 Epoch 58/100, Loss: 2.0244, Val Loss: 2.0227, F1 Score: 0.7898\n",
      "Fold 3 Epoch 59/100, Loss: 2.0242, Val Loss: 2.0228, F1 Score: 0.7899\n",
      "Fold 3 Epoch 60/100, Loss: 2.0242, Val Loss: 2.0227, F1 Score: 0.7897\n",
      "Fold 3 Epoch 61/100, Loss: 2.0241, Val Loss: 2.0227, F1 Score: 0.7895\n",
      "Fold 3 Epoch 62/100, Loss: 2.0242, Val Loss: 2.0229, F1 Score: 0.7901\n",
      "Fold 3 Epoch 63/100, Loss: 2.0242, Val Loss: 2.0229, F1 Score: 0.7897\n",
      "Fold 3 Epoch 64/100, Loss: 2.0252, Val Loss: 2.0227, F1 Score: 0.7897\n",
      "Fold 3 Epoch 65/100, Loss: 2.0241, Val Loss: 2.0227, F1 Score: 0.7896\n",
      "Fold 3 Epoch 66/100, Loss: 2.0240, Val Loss: 2.0227, F1 Score: 0.7897\n",
      "Fold 3 Epoch 67/100, Loss: 2.0243, Val Loss: 2.0229, F1 Score: 0.7897\n",
      "Fold 3 Epoch 68/100, Loss: 2.0241, Val Loss: 2.0227, F1 Score: 0.7896\n",
      "Fold 3 Epoch 69/100, Loss: 2.0242, Val Loss: 2.0227, F1 Score: 0.7897\n",
      "Fold 3 Epoch 70/100, Loss: 2.0241, Val Loss: 2.0227, F1 Score: 0.7898\n",
      "Fold 3 Epoch 71/100, Loss: 2.0240, Val Loss: 2.0228, F1 Score: 0.7899\n",
      "Fold 3 Epoch 72/100, Loss: 2.0241, Val Loss: 2.0227, F1 Score: 0.7901\n",
      "Fold 3 Epoch 73/100, Loss: 2.0251, Val Loss: 2.0227, F1 Score: 0.7899\n",
      "Fold 3 Epoch 74/100, Loss: 2.0240, Val Loss: 2.0227, F1 Score: 0.7897\n",
      "Fold 3 Epoch 75/100, Loss: 2.0240, Val Loss: 2.0227, F1 Score: 0.7899\n",
      "Fold 3 Epoch 76/100, Loss: 2.0251, Val Loss: 2.0227, F1 Score: 0.7895\n",
      "Fold 3 Epoch 77/100, Loss: 2.0251, Val Loss: 2.0227, F1 Score: 0.7898\n",
      "Fold 3 Epoch 78/100, Loss: 2.0240, Val Loss: 2.0227, F1 Score: 0.7897\n",
      "Fold 3 Epoch 79/100, Loss: 2.0241, Val Loss: 2.0229, F1 Score: 0.7898\n",
      "Fold 3 Epoch 80/100, Loss: 2.0251, Val Loss: 2.0227, F1 Score: 0.7897\n",
      "Fold 3 Epoch 81/100, Loss: 2.0239, Val Loss: 2.0227, F1 Score: 0.7900\n",
      "Fold 3 Epoch 82/100, Loss: 2.0240, Val Loss: 2.0226, F1 Score: 0.7900\n",
      "Fold 3 Epoch 83/100, Loss: 2.0240, Val Loss: 2.0226, F1 Score: 0.7899\n",
      "Fold 3 Epoch 84/100, Loss: 2.0250, Val Loss: 2.0226, F1 Score: 0.7903\n",
      "Fold 3 Epoch 85/100, Loss: 2.0239, Val Loss: 2.0226, F1 Score: 0.7899\n",
      "Fold 3 Epoch 86/100, Loss: 2.0240, Val Loss: 2.0226, F1 Score: 0.7902\n",
      "Fold 3 Epoch 87/100, Loss: 2.0239, Val Loss: 2.0226, F1 Score: 0.7900\n",
      "Fold 3 Epoch 88/100, Loss: 2.0239, Val Loss: 2.0226, F1 Score: 0.7901\n",
      "Fold 3 Epoch 89/100, Loss: 2.0239, Val Loss: 2.0225, F1 Score: 0.7899\n",
      "Fold 3 Epoch 90/100, Loss: 2.0239, Val Loss: 2.0225, F1 Score: 0.7901\n",
      "Fold 3 Epoch 91/100, Loss: 2.0238, Val Loss: 2.0225, F1 Score: 0.7898\n",
      "Fold 3 Epoch 92/100, Loss: 2.0239, Val Loss: 2.0226, F1 Score: 0.7901\n",
      "Fold 3 Epoch 93/100, Loss: 2.0238, Val Loss: 2.0225, F1 Score: 0.7900\n",
      "Fold 3 Epoch 94/100, Loss: 2.0239, Val Loss: 2.0225, F1 Score: 0.7898\n",
      "Fold 3 Epoch 95/100, Loss: 2.0239, Val Loss: 2.0226, F1 Score: 0.7902\n",
      "Fold 3 Epoch 96/100, Loss: 2.0238, Val Loss: 2.0226, F1 Score: 0.7899\n",
      "Fold 3 Epoch 97/100, Loss: 2.0249, Val Loss: 2.0225, F1 Score: 0.7898\n",
      "Fold 3 Epoch 98/100, Loss: 2.0239, Val Loss: 2.0225, F1 Score: 0.7900\n",
      "Fold 3 Epoch 99/100, Loss: 2.0248, Val Loss: 2.0225, F1 Score: 0.7899\n",
      "Fold 3 Epoch 100/100, Loss: 2.0248, Val Loss: 2.0225, F1 Score: 0.7898\n",
      "Starting Fold 4\n",
      "Fold 4 Epoch 1/100, Loss: 2.5250, Val Loss: 2.3424, F1 Score: 0.3945\n",
      "Fold 4 Epoch 2/100, Loss: 2.2749, Val Loss: 2.2479, F1 Score: 0.4591\n",
      "Fold 4 Epoch 3/100, Loss: 2.2501, Val Loss: 2.2442, F1 Score: 0.4476\n",
      "Fold 4 Epoch 4/100, Loss: 2.2475, Val Loss: 2.2441, F1 Score: 0.4520\n",
      "Fold 4 Epoch 5/100, Loss: 2.2469, Val Loss: 2.2442, F1 Score: 0.4531\n",
      "Fold 4 Epoch 6/100, Loss: 2.2462, Val Loss: 2.2434, F1 Score: 0.4487\n",
      "Fold 4 Epoch 7/100, Loss: 2.2458, Val Loss: 2.2433, F1 Score: 0.4571\n",
      "Fold 4 Epoch 8/100, Loss: 2.2453, Val Loss: 2.2429, F1 Score: 0.4577\n",
      "Fold 4 Epoch 9/100, Loss: 2.2455, Val Loss: 2.2411, F1 Score: 0.4486\n",
      "Fold 4 Epoch 10/100, Loss: 2.2229, Val Loss: 2.1849, F1 Score: 0.5665\n",
      "Fold 4 Epoch 11/100, Loss: 2.1583, Val Loss: 2.1449, F1 Score: 0.5938\n",
      "Fold 4 Epoch 12/100, Loss: 2.1468, Val Loss: 2.1435, F1 Score: 0.5952\n",
      "Fold 4 Epoch 13/100, Loss: 2.1402, Val Loss: 2.1178, F1 Score: 0.6809\n",
      "Fold 4 Epoch 14/100, Loss: 2.0953, Val Loss: 2.0854, F1 Score: 0.6989\n",
      "Fold 4 Epoch 15/100, Loss: 2.0829, Val Loss: 2.0837, F1 Score: 0.6968\n",
      "Fold 4 Epoch 16/100, Loss: 2.0810, Val Loss: 2.0824, F1 Score: 0.6975\n",
      "Fold 4 Epoch 17/100, Loss: 2.0791, Val Loss: 2.0833, F1 Score: 0.7000\n",
      "Fold 4 Epoch 18/100, Loss: 2.0787, Val Loss: 2.0820, F1 Score: 0.6983\n",
      "Fold 4 Epoch 19/100, Loss: 2.0793, Val Loss: 2.0815, F1 Score: 0.6976\n",
      "Fold 4 Epoch 20/100, Loss: 2.0780, Val Loss: 2.0814, F1 Score: 0.7015\n",
      "Fold 4 Epoch 21/100, Loss: 2.0789, Val Loss: 2.0813, F1 Score: 0.7011\n",
      "Fold 4 Epoch 22/100, Loss: 2.0785, Val Loss: 2.0810, F1 Score: 0.7008\n",
      "Fold 4 Epoch 23/100, Loss: 2.0774, Val Loss: 2.0809, F1 Score: 0.6994\n",
      "Fold 4 Epoch 24/100, Loss: 2.0781, Val Loss: 2.0805, F1 Score: 0.7019\n",
      "Fold 4 Epoch 25/100, Loss: 2.0768, Val Loss: 2.0800, F1 Score: 0.7009\n",
      "Fold 4 Epoch 26/100, Loss: 2.0762, Val Loss: 2.0787, F1 Score: 0.7020\n",
      "Fold 4 Epoch 27/100, Loss: 2.0756, Val Loss: 2.0753, F1 Score: 0.7012\n",
      "Fold 4 Epoch 28/100, Loss: 2.0658, Val Loss: 2.0584, F1 Score: 0.7407\n",
      "Fold 4 Epoch 29/100, Loss: 2.0570, Val Loss: 2.0569, F1 Score: 0.7411\n",
      "Fold 4 Epoch 30/100, Loss: 2.0549, Val Loss: 2.0559, F1 Score: 0.7414\n",
      "Fold 4 Epoch 31/100, Loss: 2.0534, Val Loss: 2.0544, F1 Score: 0.7408\n",
      "Fold 4 Epoch 32/100, Loss: 2.0507, Val Loss: 2.0496, F1 Score: 0.7584\n",
      "Fold 4 Epoch 33/100, Loss: 2.0475, Val Loss: 2.0437, F1 Score: 0.7629\n",
      "Fold 4 Epoch 34/100, Loss: 2.0422, Val Loss: 2.0374, F1 Score: 0.7812\n",
      "Fold 4 Epoch 35/100, Loss: 2.0365, Val Loss: 2.0324, F1 Score: 0.7827\n",
      "Fold 4 Epoch 36/100, Loss: 2.0330, Val Loss: 2.0319, F1 Score: 0.7817\n",
      "Fold 4 Epoch 37/100, Loss: 2.0316, Val Loss: 2.0296, F1 Score: 0.7832\n",
      "Fold 4 Epoch 38/100, Loss: 2.0289, Val Loss: 2.0290, F1 Score: 0.7831\n",
      "Fold 4 Epoch 39/100, Loss: 2.0279, Val Loss: 2.0282, F1 Score: 0.7837\n",
      "Fold 4 Epoch 40/100, Loss: 2.0266, Val Loss: 2.0283, F1 Score: 0.7832\n",
      "Fold 4 Epoch 41/100, Loss: 2.0265, Val Loss: 2.0282, F1 Score: 0.7833\n",
      "Fold 4 Epoch 42/100, Loss: 2.0259, Val Loss: 2.0279, F1 Score: 0.7839\n",
      "Fold 4 Epoch 43/100, Loss: 2.0263, Val Loss: 2.0279, F1 Score: 0.7842\n",
      "Fold 4 Epoch 44/100, Loss: 2.0261, Val Loss: 2.0279, F1 Score: 0.7838\n",
      "Fold 4 Epoch 45/100, Loss: 2.0246, Val Loss: 2.0277, F1 Score: 0.7843\n",
      "Fold 4 Epoch 46/100, Loss: 2.0245, Val Loss: 2.0275, F1 Score: 0.7846\n",
      "Fold 4 Epoch 47/100, Loss: 2.0243, Val Loss: 2.0274, F1 Score: 0.7844\n",
      "Fold 4 Epoch 48/100, Loss: 2.0241, Val Loss: 2.0273, F1 Score: 0.7846\n",
      "Fold 4 Epoch 49/100, Loss: 2.0240, Val Loss: 2.0273, F1 Score: 0.7845\n",
      "Fold 4 Epoch 50/100, Loss: 2.0250, Val Loss: 2.0272, F1 Score: 0.7846\n",
      "Fold 4 Epoch 51/100, Loss: 2.0239, Val Loss: 2.0273, F1 Score: 0.7846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 52/100, Loss: 2.0248, Val Loss: 2.0272, F1 Score: 0.7846\n",
      "Fold 4 Epoch 53/100, Loss: 2.0237, Val Loss: 2.0272, F1 Score: 0.7846\n",
      "Fold 4 Epoch 54/100, Loss: 2.0236, Val Loss: 2.0271, F1 Score: 0.7847\n",
      "Fold 4 Epoch 55/100, Loss: 2.0245, Val Loss: 2.0272, F1 Score: 0.7847\n",
      "Fold 4 Epoch 56/100, Loss: 2.0236, Val Loss: 2.0271, F1 Score: 0.7847\n",
      "Fold 4 Epoch 57/100, Loss: 2.0236, Val Loss: 2.0270, F1 Score: 0.7847\n",
      "Fold 4 Epoch 58/100, Loss: 2.0244, Val Loss: 2.0271, F1 Score: 0.7847\n",
      "Fold 4 Epoch 59/100, Loss: 2.0234, Val Loss: 2.0271, F1 Score: 0.7847\n",
      "Fold 4 Epoch 60/100, Loss: 2.0244, Val Loss: 2.0270, F1 Score: 0.7848\n",
      "Fold 4 Epoch 61/100, Loss: 2.0233, Val Loss: 2.0270, F1 Score: 0.7847\n",
      "Fold 4 Epoch 62/100, Loss: 2.0242, Val Loss: 2.0270, F1 Score: 0.7848\n",
      "Fold 4 Epoch 63/100, Loss: 2.0242, Val Loss: 2.0270, F1 Score: 0.7848\n",
      "Fold 4 Epoch 64/100, Loss: 2.0232, Val Loss: 2.0270, F1 Score: 0.7847\n",
      "Fold 4 Epoch 65/100, Loss: 2.0231, Val Loss: 2.0269, F1 Score: 0.7847\n",
      "Fold 4 Epoch 66/100, Loss: 2.0231, Val Loss: 2.0269, F1 Score: 0.7849\n",
      "Fold 4 Epoch 67/100, Loss: 2.0240, Val Loss: 2.0268, F1 Score: 0.7847\n",
      "Fold 4 Epoch 68/100, Loss: 2.0228, Val Loss: 2.0263, F1 Score: 0.7846\n",
      "Fold 4 Epoch 69/100, Loss: 2.0211, Val Loss: 2.0209, F1 Score: 0.7953\n",
      "Fold 4 Epoch 70/100, Loss: 2.0182, Val Loss: 2.0204, F1 Score: 0.7956\n",
      "Fold 4 Epoch 71/100, Loss: 2.0173, Val Loss: 2.0201, F1 Score: 0.7958\n",
      "Fold 4 Epoch 72/100, Loss: 2.0169, Val Loss: 2.0200, F1 Score: 0.7956\n",
      "Fold 4 Epoch 73/100, Loss: 2.0166, Val Loss: 2.0199, F1 Score: 0.7958\n",
      "Fold 4 Epoch 74/100, Loss: 2.0174, Val Loss: 2.0199, F1 Score: 0.7958\n",
      "Fold 4 Epoch 75/100, Loss: 2.0166, Val Loss: 2.0197, F1 Score: 0.7959\n",
      "Fold 4 Epoch 76/100, Loss: 2.0165, Val Loss: 2.0197, F1 Score: 0.7958\n",
      "Fold 4 Epoch 77/100, Loss: 2.0162, Val Loss: 2.0197, F1 Score: 0.7959\n",
      "Fold 4 Epoch 78/100, Loss: 2.0163, Val Loss: 2.0197, F1 Score: 0.7960\n",
      "Fold 4 Epoch 79/100, Loss: 2.0161, Val Loss: 2.0197, F1 Score: 0.7959\n",
      "Fold 4 Epoch 80/100, Loss: 2.0160, Val Loss: 2.0197, F1 Score: 0.7960\n",
      "Fold 4 Epoch 81/100, Loss: 2.0160, Val Loss: 2.0197, F1 Score: 0.7960\n",
      "Fold 4 Epoch 82/100, Loss: 2.0160, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 83/100, Loss: 2.0170, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 84/100, Loss: 2.0158, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 85/100, Loss: 2.0169, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 86/100, Loss: 2.0158, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 87/100, Loss: 2.0158, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 88/100, Loss: 2.0168, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 89/100, Loss: 2.0158, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 90/100, Loss: 2.0159, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 91/100, Loss: 2.0157, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 92/100, Loss: 2.0167, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 93/100, Loss: 2.0157, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 94/100, Loss: 2.0157, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 95/100, Loss: 2.0157, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 96/100, Loss: 2.0157, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 97/100, Loss: 2.0157, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 98/100, Loss: 2.0156, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 99/100, Loss: 2.0157, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Fold 4 Epoch 100/100, Loss: 2.0158, Val Loss: 2.0196, F1 Score: 0.7959\n",
      "Starting Fold 5\n",
      "Fold 5 Epoch 1/100, Loss: 2.5448, Val Loss: 2.3587, F1 Score: 0.3950\n",
      "Fold 5 Epoch 2/100, Loss: 2.2790, Val Loss: 2.2519, F1 Score: 0.4468\n",
      "Fold 5 Epoch 3/100, Loss: 2.2503, Val Loss: 2.2483, F1 Score: 0.4440\n",
      "Fold 5 Epoch 4/100, Loss: 2.2484, Val Loss: 2.2481, F1 Score: 0.4520\n",
      "Fold 5 Epoch 5/100, Loss: 2.2465, Val Loss: 2.2471, F1 Score: 0.4492\n",
      "Fold 5 Epoch 6/100, Loss: 2.2459, Val Loss: 2.2470, F1 Score: 0.4514\n",
      "Fold 5 Epoch 7/100, Loss: 2.2454, Val Loss: 2.2470, F1 Score: 0.4587\n",
      "Fold 5 Epoch 8/100, Loss: 2.2447, Val Loss: 2.2460, F1 Score: 0.4528\n",
      "Fold 5 Epoch 9/100, Loss: 2.2407, Val Loss: 2.2269, F1 Score: 0.4400\n",
      "Fold 5 Epoch 10/100, Loss: 2.1774, Val Loss: 2.1481, F1 Score: 0.5914\n",
      "Fold 5 Epoch 11/100, Loss: 2.1449, Val Loss: 2.1202, F1 Score: 0.6796\n",
      "Fold 5 Epoch 12/100, Loss: 2.0981, Val Loss: 2.0841, F1 Score: 0.6998\n",
      "Fold 5 Epoch 13/100, Loss: 2.0829, Val Loss: 2.0828, F1 Score: 0.7039\n",
      "Fold 5 Epoch 14/100, Loss: 2.0806, Val Loss: 2.0793, F1 Score: 0.7008\n",
      "Fold 5 Epoch 15/100, Loss: 2.0806, Val Loss: 2.0791, F1 Score: 0.7014\n",
      "Fold 5 Epoch 16/100, Loss: 2.0791, Val Loss: 2.0790, F1 Score: 0.7043\n",
      "Fold 5 Epoch 17/100, Loss: 2.0785, Val Loss: 2.0787, F1 Score: 0.7026\n",
      "Fold 5 Epoch 18/100, Loss: 2.0781, Val Loss: 2.0786, F1 Score: 0.7012\n",
      "Fold 5 Epoch 19/100, Loss: 2.0789, Val Loss: 2.0784, F1 Score: 0.7031\n",
      "Fold 5 Epoch 20/100, Loss: 2.0778, Val Loss: 2.0781, F1 Score: 0.7049\n",
      "Fold 5 Epoch 21/100, Loss: 2.0772, Val Loss: 2.0774, F1 Score: 0.7047\n",
      "Fold 5 Epoch 22/100, Loss: 2.0761, Val Loss: 2.0745, F1 Score: 0.7045\n",
      "Fold 5 Epoch 23/100, Loss: 2.0666, Val Loss: 2.0565, F1 Score: 0.7442\n",
      "Fold 5 Epoch 24/100, Loss: 2.0590, Val Loss: 2.0551, F1 Score: 0.7439\n",
      "Fold 5 Epoch 25/100, Loss: 2.0560, Val Loss: 2.0552, F1 Score: 0.7420\n",
      "Fold 5 Epoch 26/100, Loss: 2.0551, Val Loss: 2.0545, F1 Score: 0.7418\n",
      "Fold 5 Epoch 27/100, Loss: 2.0546, Val Loss: 2.0545, F1 Score: 0.7417\n",
      "Fold 5 Epoch 28/100, Loss: 2.0544, Val Loss: 2.0545, F1 Score: 0.7420\n",
      "Fold 5 Epoch 29/100, Loss: 2.0551, Val Loss: 2.0542, F1 Score: 0.7418\n",
      "Fold 5 Epoch 30/100, Loss: 2.0536, Val Loss: 2.0540, F1 Score: 0.7425\n",
      "Fold 5 Epoch 31/100, Loss: 2.0533, Val Loss: 2.0536, F1 Score: 0.7436\n",
      "Fold 5 Epoch 32/100, Loss: 2.0523, Val Loss: 2.0501, F1 Score: 0.7450\n",
      "Fold 5 Epoch 33/100, Loss: 2.0486, Val Loss: 2.0435, F1 Score: 0.7659\n",
      "Fold 5 Epoch 34/100, Loss: 2.0449, Val Loss: 2.0417, F1 Score: 0.7661\n",
      "Fold 5 Epoch 35/100, Loss: 2.0438, Val Loss: 2.0406, F1 Score: 0.7662\n",
      "Fold 5 Epoch 36/100, Loss: 2.0408, Val Loss: 2.0378, F1 Score: 0.7672\n",
      "Fold 5 Epoch 37/100, Loss: 2.0368, Val Loss: 2.0316, F1 Score: 0.7832\n",
      "Fold 5 Epoch 38/100, Loss: 2.0328, Val Loss: 2.0284, F1 Score: 0.7856\n",
      "Fold 5 Epoch 39/100, Loss: 2.0301, Val Loss: 2.0273, F1 Score: 0.7859\n",
      "Fold 5 Epoch 40/100, Loss: 2.0289, Val Loss: 2.0275, F1 Score: 0.7856\n",
      "Fold 5 Epoch 41/100, Loss: 2.0275, Val Loss: 2.0272, F1 Score: 0.7862\n",
      "Fold 5 Epoch 42/100, Loss: 2.0269, Val Loss: 2.0267, F1 Score: 0.7861\n",
      "Fold 5 Epoch 43/100, Loss: 2.0261, Val Loss: 2.0265, F1 Score: 0.7866\n",
      "Fold 5 Epoch 44/100, Loss: 2.0256, Val Loss: 2.0258, F1 Score: 0.7871\n",
      "Fold 5 Epoch 45/100, Loss: 2.0263, Val Loss: 2.0260, F1 Score: 0.7871\n",
      "Fold 5 Epoch 46/100, Loss: 2.0248, Val Loss: 2.0258, F1 Score: 0.7870\n",
      "Fold 5 Epoch 47/100, Loss: 2.0247, Val Loss: 2.0258, F1 Score: 0.7871\n",
      "Fold 5 Epoch 48/100, Loss: 2.0256, Val Loss: 2.0258, F1 Score: 0.7871\n",
      "Fold 5 Epoch 49/100, Loss: 2.0244, Val Loss: 2.0258, F1 Score: 0.7870\n",
      "Fold 5 Epoch 50/100, Loss: 2.0242, Val Loss: 2.0257, F1 Score: 0.7872\n",
      "Fold 5 Epoch 51/100, Loss: 2.0244, Val Loss: 2.0257, F1 Score: 0.7871\n",
      "Fold 5 Epoch 52/100, Loss: 2.0242, Val Loss: 2.0257, F1 Score: 0.7871\n",
      "Fold 5 Epoch 53/100, Loss: 2.0242, Val Loss: 2.0258, F1 Score: 0.7870\n",
      "Fold 5 Epoch 54/100, Loss: 2.0242, Val Loss: 2.0260, F1 Score: 0.7873\n",
      "Fold 5 Epoch 55/100, Loss: 2.0241, Val Loss: 2.0257, F1 Score: 0.7874\n",
      "Fold 5 Epoch 56/100, Loss: 2.0240, Val Loss: 2.0257, F1 Score: 0.7872\n",
      "Fold 5 Epoch 57/100, Loss: 2.0249, Val Loss: 2.0255, F1 Score: 0.7875\n",
      "Fold 5 Epoch 58/100, Loss: 2.0237, Val Loss: 2.0257, F1 Score: 0.7872\n",
      "Fold 5 Epoch 59/100, Loss: 2.0236, Val Loss: 2.0257, F1 Score: 0.7872\n",
      "Fold 5 Epoch 60/100, Loss: 2.0236, Val Loss: 2.0256, F1 Score: 0.7871\n",
      "Fold 5 Epoch 61/100, Loss: 2.0249, Val Loss: 2.0255, F1 Score: 0.7875\n",
      "Fold 5 Epoch 62/100, Loss: 2.0237, Val Loss: 2.0255, F1 Score: 0.7873\n",
      "Fold 5 Epoch 63/100, Loss: 2.0235, Val Loss: 2.0256, F1 Score: 0.7873\n",
      "Fold 5 Epoch 64/100, Loss: 2.0237, Val Loss: 2.0255, F1 Score: 0.7875\n",
      "Fold 5 Epoch 65/100, Loss: 2.0236, Val Loss: 2.0254, F1 Score: 0.7873\n",
      "Fold 5 Epoch 66/100, Loss: 2.0235, Val Loss: 2.0255, F1 Score: 0.7874\n",
      "Fold 5 Epoch 67/100, Loss: 2.0235, Val Loss: 2.0255, F1 Score: 0.7874\n",
      "Fold 5 Epoch 68/100, Loss: 2.0244, Val Loss: 2.0255, F1 Score: 0.7873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 69/100, Loss: 2.0246, Val Loss: 2.0255, F1 Score: 0.7873\n",
      "Fold 5 Epoch 70/100, Loss: 2.0235, Val Loss: 2.0255, F1 Score: 0.7873\n",
      "Fold 5 Epoch 71/100, Loss: 2.0234, Val Loss: 2.0255, F1 Score: 0.7873\n",
      "Fold 5 Epoch 72/100, Loss: 2.0246, Val Loss: 2.0255, F1 Score: 0.7874\n",
      "Fold 5 Epoch 73/100, Loss: 2.0233, Val Loss: 2.0255, F1 Score: 0.7874\n",
      "Fold 5 Epoch 74/100, Loss: 2.0245, Val Loss: 2.0256, F1 Score: 0.7873\n",
      "Fold 5 Epoch 75/100, Loss: 2.0234, Val Loss: 2.0255, F1 Score: 0.7874\n",
      "Fold 5 Epoch 76/100, Loss: 2.0244, Val Loss: 2.0254, F1 Score: 0.7876\n",
      "Fold 5 Epoch 77/100, Loss: 2.0235, Val Loss: 2.0253, F1 Score: 0.7876\n",
      "Fold 5 Epoch 78/100, Loss: 2.0244, Val Loss: 2.0254, F1 Score: 0.7874\n",
      "Fold 5 Epoch 79/100, Loss: 2.0234, Val Loss: 2.0254, F1 Score: 0.7874\n",
      "Fold 5 Epoch 80/100, Loss: 2.0243, Val Loss: 2.0254, F1 Score: 0.7876\n",
      "Fold 5 Epoch 81/100, Loss: 2.0233, Val Loss: 2.0254, F1 Score: 0.7874\n",
      "Fold 5 Epoch 82/100, Loss: 2.0232, Val Loss: 2.0253, F1 Score: 0.7874\n",
      "Fold 5 Epoch 83/100, Loss: 2.0232, Val Loss: 2.0253, F1 Score: 0.7875\n",
      "Fold 5 Epoch 84/100, Loss: 2.0231, Val Loss: 2.0251, F1 Score: 0.7873\n",
      "Fold 5 Epoch 85/100, Loss: 2.0240, Val Loss: 2.0250, F1 Score: 0.7875\n",
      "Fold 5 Epoch 86/100, Loss: 2.0239, Val Loss: 2.0246, F1 Score: 0.7874\n",
      "Fold 5 Epoch 87/100, Loss: 2.0232, Val Loss: 2.0229, F1 Score: 0.7947\n",
      "Fold 5 Epoch 88/100, Loss: 2.0207, Val Loss: 2.0209, F1 Score: 0.7980\n",
      "Fold 5 Epoch 89/100, Loss: 2.0195, Val Loss: 2.0198, F1 Score: 0.7982\n",
      "Fold 5 Epoch 90/100, Loss: 2.0185, Val Loss: 2.0193, F1 Score: 0.7979\n",
      "Fold 5 Epoch 91/100, Loss: 2.0175, Val Loss: 2.0187, F1 Score: 0.7983\n",
      "Fold 5 Epoch 92/100, Loss: 2.0183, Val Loss: 2.0185, F1 Score: 0.7985\n",
      "Fold 5 Epoch 93/100, Loss: 2.0169, Val Loss: 2.0183, F1 Score: 0.7985\n",
      "Fold 5 Epoch 94/100, Loss: 2.0169, Val Loss: 2.0186, F1 Score: 0.7980\n",
      "Fold 5 Epoch 95/100, Loss: 2.0168, Val Loss: 2.0184, F1 Score: 0.7983\n",
      "Fold 5 Epoch 96/100, Loss: 2.0167, Val Loss: 2.0183, F1 Score: 0.7983\n",
      "Fold 5 Epoch 97/100, Loss: 2.0164, Val Loss: 2.0183, F1 Score: 0.7983\n",
      "Fold 5 Epoch 98/100, Loss: 2.0166, Val Loss: 2.0186, F1 Score: 0.7981\n",
      "Fold 5 Epoch 99/100, Loss: 2.0164, Val Loss: 2.0181, F1 Score: 0.7985\n",
      "Fold 5 Epoch 100/100, Loss: 2.0163, Val Loss: 2.0183, F1 Score: 0.7983\n",
      "Starting Fold 6\n",
      "Fold 6 Epoch 1/100, Loss: 2.5451, Val Loss: 2.3469, F1 Score: 0.3991\n",
      "Fold 6 Epoch 2/100, Loss: 2.2770, Val Loss: 2.2470, F1 Score: 0.4484\n",
      "Fold 6 Epoch 3/100, Loss: 2.2516, Val Loss: 2.2438, F1 Score: 0.4477\n",
      "Fold 6 Epoch 4/100, Loss: 2.2479, Val Loss: 2.2435, F1 Score: 0.4611\n",
      "Fold 6 Epoch 5/100, Loss: 2.2467, Val Loss: 2.2422, F1 Score: 0.4558\n",
      "Fold 6 Epoch 6/100, Loss: 2.2459, Val Loss: 2.2415, F1 Score: 0.4560\n",
      "Fold 6 Epoch 7/100, Loss: 2.2445, Val Loss: 2.2387, F1 Score: 0.4543\n",
      "Fold 6 Epoch 8/100, Loss: 2.2211, Val Loss: 2.1539, F1 Score: 0.5944\n",
      "Fold 6 Epoch 9/100, Loss: 2.1549, Val Loss: 2.1357, F1 Score: 0.5968\n",
      "Fold 6 Epoch 10/100, Loss: 2.1154, Val Loss: 2.0869, F1 Score: 0.6906\n",
      "Fold 6 Epoch 11/100, Loss: 2.0879, Val Loss: 2.0783, F1 Score: 0.7012\n",
      "Fold 6 Epoch 12/100, Loss: 2.0817, Val Loss: 2.0770, F1 Score: 0.7031\n",
      "Fold 6 Epoch 13/100, Loss: 2.0805, Val Loss: 2.0764, F1 Score: 0.7054\n",
      "Fold 6 Epoch 14/100, Loss: 2.0807, Val Loss: 2.0757, F1 Score: 0.7043\n",
      "Fold 6 Epoch 15/100, Loss: 2.0791, Val Loss: 2.0755, F1 Score: 0.7041\n",
      "Fold 6 Epoch 16/100, Loss: 2.0787, Val Loss: 2.0753, F1 Score: 0.7055\n",
      "Fold 6 Epoch 17/100, Loss: 2.0784, Val Loss: 2.0750, F1 Score: 0.7056\n",
      "Fold 6 Epoch 18/100, Loss: 2.0780, Val Loss: 2.0745, F1 Score: 0.7068\n",
      "Fold 6 Epoch 19/100, Loss: 2.0773, Val Loss: 2.0733, F1 Score: 0.7075\n",
      "Fold 6 Epoch 20/100, Loss: 2.0752, Val Loss: 2.0682, F1 Score: 0.7116\n",
      "Fold 6 Epoch 21/100, Loss: 2.0634, Val Loss: 2.0541, F1 Score: 0.7447\n",
      "Fold 6 Epoch 22/100, Loss: 2.0574, Val Loss: 2.0533, F1 Score: 0.7448\n",
      "Fold 6 Epoch 23/100, Loss: 2.0569, Val Loss: 2.0518, F1 Score: 0.7437\n",
      "Fold 6 Epoch 24/100, Loss: 2.0540, Val Loss: 2.0472, F1 Score: 0.7631\n",
      "Fold 6 Epoch 25/100, Loss: 2.0484, Val Loss: 2.0412, F1 Score: 0.7657\n",
      "Fold 6 Epoch 26/100, Loss: 2.0436, Val Loss: 2.0367, F1 Score: 0.7802\n",
      "Fold 6 Epoch 27/100, Loss: 2.0378, Val Loss: 2.0292, F1 Score: 0.7848\n",
      "Fold 6 Epoch 28/100, Loss: 2.0331, Val Loss: 2.0272, F1 Score: 0.7854\n",
      "Fold 6 Epoch 29/100, Loss: 2.0311, Val Loss: 2.0269, F1 Score: 0.7856\n",
      "Fold 6 Epoch 30/100, Loss: 2.0296, Val Loss: 2.0268, F1 Score: 0.7856\n",
      "Fold 6 Epoch 31/100, Loss: 2.0281, Val Loss: 2.0258, F1 Score: 0.7864\n",
      "Fold 6 Epoch 32/100, Loss: 2.0280, Val Loss: 2.0259, F1 Score: 0.7869\n",
      "Fold 6 Epoch 33/100, Loss: 2.0273, Val Loss: 2.0258, F1 Score: 0.7866\n",
      "Fold 6 Epoch 34/100, Loss: 2.0279, Val Loss: 2.0255, F1 Score: 0.7866\n",
      "Fold 6 Epoch 35/100, Loss: 2.0262, Val Loss: 2.0256, F1 Score: 0.7864\n",
      "Fold 6 Epoch 36/100, Loss: 2.0259, Val Loss: 2.0252, F1 Score: 0.7866\n",
      "Fold 6 Epoch 37/100, Loss: 2.0256, Val Loss: 2.0253, F1 Score: 0.7869\n",
      "Fold 6 Epoch 38/100, Loss: 2.0256, Val Loss: 2.0253, F1 Score: 0.7869\n",
      "Fold 6 Epoch 39/100, Loss: 2.0263, Val Loss: 2.0252, F1 Score: 0.7866\n",
      "Fold 6 Epoch 40/100, Loss: 2.0248, Val Loss: 2.0250, F1 Score: 0.7870\n",
      "Fold 6 Epoch 41/100, Loss: 2.0248, Val Loss: 2.0250, F1 Score: 0.7870\n",
      "Fold 6 Epoch 42/100, Loss: 2.0245, Val Loss: 2.0251, F1 Score: 0.7871\n",
      "Fold 6 Epoch 43/100, Loss: 2.0245, Val Loss: 2.0251, F1 Score: 0.7871\n",
      "Fold 6 Epoch 44/100, Loss: 2.0244, Val Loss: 2.0249, F1 Score: 0.7870\n",
      "Fold 6 Epoch 45/100, Loss: 2.0243, Val Loss: 2.0249, F1 Score: 0.7870\n",
      "Fold 6 Epoch 46/100, Loss: 2.0251, Val Loss: 2.0250, F1 Score: 0.7871\n",
      "Fold 6 Epoch 47/100, Loss: 2.0241, Val Loss: 2.0249, F1 Score: 0.7870\n",
      "Fold 6 Epoch 48/100, Loss: 2.0241, Val Loss: 2.0248, F1 Score: 0.7870\n",
      "Fold 6 Epoch 49/100, Loss: 2.0241, Val Loss: 2.0249, F1 Score: 0.7871\n",
      "Fold 6 Epoch 50/100, Loss: 2.0240, Val Loss: 2.0250, F1 Score: 0.7871\n",
      "Fold 6 Epoch 51/100, Loss: 2.0239, Val Loss: 2.0247, F1 Score: 0.7871\n",
      "Fold 6 Epoch 52/100, Loss: 2.0239, Val Loss: 2.0248, F1 Score: 0.7871\n",
      "Fold 6 Epoch 53/100, Loss: 2.0238, Val Loss: 2.0249, F1 Score: 0.7871\n",
      "Fold 6 Epoch 54/100, Loss: 2.0248, Val Loss: 2.0248, F1 Score: 0.7870\n",
      "Fold 6 Epoch 55/100, Loss: 2.0238, Val Loss: 2.0245, F1 Score: 0.7872\n",
      "Fold 6 Epoch 56/100, Loss: 2.0237, Val Loss: 2.0248, F1 Score: 0.7869\n",
      "Fold 6 Epoch 57/100, Loss: 2.0237, Val Loss: 2.0246, F1 Score: 0.7871\n",
      "Fold 6 Epoch 58/100, Loss: 2.0247, Val Loss: 2.0249, F1 Score: 0.7872\n",
      "Fold 6 Epoch 59/100, Loss: 2.0236, Val Loss: 2.0247, F1 Score: 0.7870\n",
      "Fold 6 Epoch 60/100, Loss: 2.0236, Val Loss: 2.0248, F1 Score: 0.7871\n",
      "Fold 6 Epoch 61/100, Loss: 2.0236, Val Loss: 2.0247, F1 Score: 0.7871\n",
      "Fold 6 Epoch 62/100, Loss: 2.0234, Val Loss: 2.0245, F1 Score: 0.7872\n",
      "Fold 6 Epoch 63/100, Loss: 2.0244, Val Loss: 2.0245, F1 Score: 0.7872\n",
      "Fold 6 Epoch 64/100, Loss: 2.0233, Val Loss: 2.0243, F1 Score: 0.7871\n",
      "Fold 6 Epoch 65/100, Loss: 2.0240, Val Loss: 2.0236, F1 Score: 0.7871\n",
      "Fold 6 Epoch 66/100, Loss: 2.0230, Val Loss: 2.0213, F1 Score: 0.7965\n",
      "Fold 6 Epoch 67/100, Loss: 2.0199, Val Loss: 2.0187, F1 Score: 0.7974\n",
      "Fold 6 Epoch 68/100, Loss: 2.0184, Val Loss: 2.0189, F1 Score: 0.7972\n",
      "Fold 6 Epoch 69/100, Loss: 2.0188, Val Loss: 2.0184, F1 Score: 0.7976\n",
      "Fold 6 Epoch 70/100, Loss: 2.0173, Val Loss: 2.0181, F1 Score: 0.7977\n",
      "Fold 6 Epoch 71/100, Loss: 2.0181, Val Loss: 2.0180, F1 Score: 0.7980\n",
      "Fold 6 Epoch 72/100, Loss: 2.0180, Val Loss: 2.0181, F1 Score: 0.7978\n",
      "Fold 6 Epoch 73/100, Loss: 2.0179, Val Loss: 2.0181, F1 Score: 0.7979\n",
      "Fold 6 Epoch 74/100, Loss: 2.0178, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Fold 6 Epoch 75/100, Loss: 2.0167, Val Loss: 2.0179, F1 Score: 0.7979\n",
      "Fold 6 Epoch 76/100, Loss: 2.0168, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Fold 6 Epoch 77/100, Loss: 2.0167, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Fold 6 Epoch 78/100, Loss: 2.0166, Val Loss: 2.0181, F1 Score: 0.7977\n",
      "Fold 6 Epoch 79/100, Loss: 2.0164, Val Loss: 2.0180, F1 Score: 0.7977\n",
      "Fold 6 Epoch 80/100, Loss: 2.0166, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Fold 6 Epoch 81/100, Loss: 2.0164, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Fold 6 Epoch 82/100, Loss: 2.0165, Val Loss: 2.0180, F1 Score: 0.7978\n",
      "Fold 6 Epoch 83/100, Loss: 2.0174, Val Loss: 2.0180, F1 Score: 0.7978\n",
      "Fold 6 Epoch 84/100, Loss: 2.0165, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Fold 6 Epoch 85/100, Loss: 2.0163, Val Loss: 2.0179, F1 Score: 0.7979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 Epoch 86/100, Loss: 2.0165, Val Loss: 2.0182, F1 Score: 0.7977\n",
      "Fold 6 Epoch 87/100, Loss: 2.0174, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Fold 6 Epoch 88/100, Loss: 2.0163, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Fold 6 Epoch 89/100, Loss: 2.0163, Val Loss: 2.0179, F1 Score: 0.7979\n",
      "Fold 6 Epoch 90/100, Loss: 2.0173, Val Loss: 2.0179, F1 Score: 0.7979\n",
      "Fold 6 Epoch 91/100, Loss: 2.0163, Val Loss: 2.0186, F1 Score: 0.7975\n",
      "Fold 6 Epoch 92/100, Loss: 2.0165, Val Loss: 2.0181, F1 Score: 0.7979\n",
      "Fold 6 Epoch 93/100, Loss: 2.0173, Val Loss: 2.0179, F1 Score: 0.7979\n",
      "Fold 6 Epoch 94/100, Loss: 2.0162, Val Loss: 2.0180, F1 Score: 0.7978\n",
      "Fold 6 Epoch 95/100, Loss: 2.0163, Val Loss: 2.0181, F1 Score: 0.7978\n",
      "Fold 6 Epoch 96/100, Loss: 2.0162, Val Loss: 2.0181, F1 Score: 0.7978\n",
      "Fold 6 Epoch 97/100, Loss: 2.0173, Val Loss: 2.0180, F1 Score: 0.7978\n",
      "Fold 6 Epoch 98/100, Loss: 2.0162, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Fold 6 Epoch 99/100, Loss: 2.0161, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Fold 6 Epoch 100/100, Loss: 2.0162, Val Loss: 2.0180, F1 Score: 0.7979\n",
      "Starting Fold 7\n",
      "Fold 7 Epoch 1/100, Loss: 2.5318, Val Loss: 2.3313, F1 Score: 0.4029\n",
      "Fold 7 Epoch 2/100, Loss: 2.2709, Val Loss: 2.2556, F1 Score: 0.4379\n",
      "Fold 7 Epoch 3/100, Loss: 2.2488, Val Loss: 2.2526, F1 Score: 0.4496\n",
      "Fold 7 Epoch 4/100, Loss: 2.2476, Val Loss: 2.2521, F1 Score: 0.4469\n",
      "Fold 7 Epoch 5/100, Loss: 2.2456, Val Loss: 2.2513, F1 Score: 0.4455\n",
      "Fold 7 Epoch 6/100, Loss: 2.2451, Val Loss: 2.2512, F1 Score: 0.4504\n",
      "Fold 7 Epoch 7/100, Loss: 2.2441, Val Loss: 2.2497, F1 Score: 0.4475\n",
      "Fold 7 Epoch 8/100, Loss: 2.2282, Val Loss: 2.1703, F1 Score: 0.5897\n",
      "Fold 7 Epoch 9/100, Loss: 2.1565, Val Loss: 2.1502, F1 Score: 0.5899\n",
      "Fold 7 Epoch 10/100, Loss: 2.1464, Val Loss: 2.1461, F1 Score: 0.5898\n",
      "Fold 7 Epoch 11/100, Loss: 2.1239, Val Loss: 2.0913, F1 Score: 0.6900\n",
      "Fold 7 Epoch 12/100, Loss: 2.0883, Val Loss: 2.0814, F1 Score: 0.7012\n",
      "Fold 7 Epoch 13/100, Loss: 2.0814, Val Loss: 2.0799, F1 Score: 0.7013\n",
      "Fold 7 Epoch 14/100, Loss: 2.0801, Val Loss: 2.0795, F1 Score: 0.7022\n",
      "Fold 7 Epoch 15/100, Loss: 2.0793, Val Loss: 2.0791, F1 Score: 0.7016\n",
      "Fold 7 Epoch 16/100, Loss: 2.0787, Val Loss: 2.0791, F1 Score: 0.7031\n",
      "Fold 7 Epoch 17/100, Loss: 2.0783, Val Loss: 2.0792, F1 Score: 0.7040\n",
      "Fold 7 Epoch 18/100, Loss: 2.0781, Val Loss: 2.0788, F1 Score: 0.7038\n",
      "Fold 7 Epoch 19/100, Loss: 2.0789, Val Loss: 2.0787, F1 Score: 0.7034\n",
      "Fold 7 Epoch 20/100, Loss: 2.0778, Val Loss: 2.0785, F1 Score: 0.7044\n",
      "Fold 7 Epoch 21/100, Loss: 2.0774, Val Loss: 2.0782, F1 Score: 0.7043\n",
      "Fold 7 Epoch 22/100, Loss: 2.0770, Val Loss: 2.0776, F1 Score: 0.7043\n",
      "Fold 7 Epoch 23/100, Loss: 2.0763, Val Loss: 2.0763, F1 Score: 0.7056\n",
      "Fold 7 Epoch 24/100, Loss: 2.0737, Val Loss: 2.0678, F1 Score: 0.7428\n",
      "Fold 7 Epoch 25/100, Loss: 2.0626, Val Loss: 2.0544, F1 Score: 0.7453\n",
      "Fold 7 Epoch 26/100, Loss: 2.0565, Val Loss: 2.0534, F1 Score: 0.7435\n",
      "Fold 7 Epoch 27/100, Loss: 2.0561, Val Loss: 2.0509, F1 Score: 0.7440\n",
      "Fold 7 Epoch 28/100, Loss: 2.0510, Val Loss: 2.0446, F1 Score: 0.7663\n",
      "Fold 7 Epoch 29/100, Loss: 2.0469, Val Loss: 2.0419, F1 Score: 0.7679\n",
      "Fold 7 Epoch 30/100, Loss: 2.0454, Val Loss: 2.0409, F1 Score: 0.7673\n",
      "Fold 7 Epoch 31/100, Loss: 2.0430, Val Loss: 2.0398, F1 Score: 0.7677\n",
      "Fold 7 Epoch 32/100, Loss: 2.0421, Val Loss: 2.0394, F1 Score: 0.7671\n",
      "Fold 7 Epoch 33/100, Loss: 2.0425, Val Loss: 2.0393, F1 Score: 0.7667\n",
      "Fold 7 Epoch 34/100, Loss: 2.0410, Val Loss: 2.0393, F1 Score: 0.7679\n",
      "Fold 7 Epoch 35/100, Loss: 2.0406, Val Loss: 2.0392, F1 Score: 0.7674\n",
      "Fold 7 Epoch 36/100, Loss: 2.0412, Val Loss: 2.0391, F1 Score: 0.7667\n",
      "Fold 7 Epoch 37/100, Loss: 2.0401, Val Loss: 2.0390, F1 Score: 0.7667\n",
      "Fold 7 Epoch 38/100, Loss: 2.0398, Val Loss: 2.0392, F1 Score: 0.7675\n",
      "Fold 7 Epoch 39/100, Loss: 2.0396, Val Loss: 2.0391, F1 Score: 0.7666\n",
      "Fold 7 Epoch 40/100, Loss: 2.0394, Val Loss: 2.0393, F1 Score: 0.7673\n",
      "Fold 7 Epoch 41/100, Loss: 2.0403, Val Loss: 2.0390, F1 Score: 0.7659\n",
      "Fold 7 Epoch 42/100, Loss: 2.0392, Val Loss: 2.0389, F1 Score: 0.7666\n",
      "Fold 7 Epoch 43/100, Loss: 2.0393, Val Loss: 2.0391, F1 Score: 0.7671\n",
      "Fold 7 Epoch 44/100, Loss: 2.0391, Val Loss: 2.0389, F1 Score: 0.7662\n",
      "Fold 7 Epoch 45/100, Loss: 2.0400, Val Loss: 2.0390, F1 Score: 0.7673\n",
      "Fold 7 Epoch 46/100, Loss: 2.0392, Val Loss: 2.0389, F1 Score: 0.7667\n",
      "Fold 7 Epoch 47/100, Loss: 2.0390, Val Loss: 2.0389, F1 Score: 0.7666\n",
      "Fold 7 Epoch 48/100, Loss: 2.0389, Val Loss: 2.0391, F1 Score: 0.7673\n",
      "Fold 7 Epoch 49/100, Loss: 2.0389, Val Loss: 2.0389, F1 Score: 0.7668\n",
      "Fold 7 Epoch 50/100, Loss: 2.0397, Val Loss: 2.0388, F1 Score: 0.7666\n",
      "Fold 7 Epoch 51/100, Loss: 2.0388, Val Loss: 2.0388, F1 Score: 0.7674\n",
      "Fold 7 Epoch 52/100, Loss: 2.0388, Val Loss: 2.0388, F1 Score: 0.7674\n",
      "Fold 7 Epoch 53/100, Loss: 2.0397, Val Loss: 2.0387, F1 Score: 0.7664\n",
      "Fold 7 Epoch 54/100, Loss: 2.0385, Val Loss: 2.0386, F1 Score: 0.7665\n",
      "Fold 7 Epoch 55/100, Loss: 2.0395, Val Loss: 2.0384, F1 Score: 0.7677\n",
      "Fold 7 Epoch 56/100, Loss: 2.0381, Val Loss: 2.0377, F1 Score: 0.7667\n",
      "Fold 7 Epoch 57/100, Loss: 2.0378, Val Loss: 2.0335, F1 Score: 0.7821\n",
      "Fold 7 Epoch 58/100, Loss: 2.0311, Val Loss: 2.0258, F1 Score: 0.7890\n",
      "Fold 7 Epoch 59/100, Loss: 2.0269, Val Loss: 2.0242, F1 Score: 0.7890\n",
      "Fold 7 Epoch 60/100, Loss: 2.0260, Val Loss: 2.0223, F1 Score: 0.7982\n",
      "Fold 7 Epoch 61/100, Loss: 2.0223, Val Loss: 2.0203, F1 Score: 0.7989\n",
      "Fold 7 Epoch 62/100, Loss: 2.0208, Val Loss: 2.0192, F1 Score: 0.7990\n",
      "Fold 7 Epoch 63/100, Loss: 2.0204, Val Loss: 2.0184, F1 Score: 0.7992\n",
      "Fold 7 Epoch 64/100, Loss: 2.0190, Val Loss: 2.0177, F1 Score: 0.8000\n",
      "Fold 7 Epoch 65/100, Loss: 2.0192, Val Loss: 2.0174, F1 Score: 0.8000\n",
      "Fold 7 Epoch 66/100, Loss: 2.0188, Val Loss: 2.0174, F1 Score: 0.8002\n",
      "Fold 7 Epoch 67/100, Loss: 2.0177, Val Loss: 2.0173, F1 Score: 0.8003\n",
      "Fold 7 Epoch 68/100, Loss: 2.0175, Val Loss: 2.0174, F1 Score: 0.8001\n",
      "Fold 7 Epoch 69/100, Loss: 2.0173, Val Loss: 2.0172, F1 Score: 0.8003\n",
      "Fold 7 Epoch 70/100, Loss: 2.0197, Val Loss: 2.0172, F1 Score: 0.8003\n",
      "Fold 7 Epoch 71/100, Loss: 2.0173, Val Loss: 2.0172, F1 Score: 0.8004\n",
      "Fold 7 Epoch 72/100, Loss: 2.0183, Val Loss: 2.0172, F1 Score: 0.8003\n",
      "Fold 7 Epoch 73/100, Loss: 2.0170, Val Loss: 2.0173, F1 Score: 0.8002\n",
      "Fold 7 Epoch 74/100, Loss: 2.0169, Val Loss: 2.0171, F1 Score: 0.8004\n",
      "Fold 7 Epoch 75/100, Loss: 2.0169, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 76/100, Loss: 2.0168, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 77/100, Loss: 2.0167, Val Loss: 2.0174, F1 Score: 0.8001\n",
      "Fold 7 Epoch 78/100, Loss: 2.0166, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 79/100, Loss: 2.0165, Val Loss: 2.0172, F1 Score: 0.8004\n",
      "Fold 7 Epoch 80/100, Loss: 2.0164, Val Loss: 2.0175, F1 Score: 0.8002\n",
      "Fold 7 Epoch 81/100, Loss: 2.0163, Val Loss: 2.0172, F1 Score: 0.8002\n",
      "Fold 7 Epoch 82/100, Loss: 2.0164, Val Loss: 2.0171, F1 Score: 0.8004\n",
      "Fold 7 Epoch 83/100, Loss: 2.0164, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 84/100, Loss: 2.0163, Val Loss: 2.0171, F1 Score: 0.8004\n",
      "Fold 7 Epoch 85/100, Loss: 2.0164, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 86/100, Loss: 2.0163, Val Loss: 2.0171, F1 Score: 0.8004\n",
      "Fold 7 Epoch 87/100, Loss: 2.0172, Val Loss: 2.0171, F1 Score: 0.8004\n",
      "Fold 7 Epoch 88/100, Loss: 2.0162, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 89/100, Loss: 2.0162, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 90/100, Loss: 2.0162, Val Loss: 2.0171, F1 Score: 0.8004\n",
      "Fold 7 Epoch 91/100, Loss: 2.0161, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 92/100, Loss: 2.0161, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 93/100, Loss: 2.0162, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 94/100, Loss: 2.0162, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 95/100, Loss: 2.0160, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 96/100, Loss: 2.0161, Val Loss: 2.0171, F1 Score: 0.8004\n",
      "Fold 7 Epoch 97/100, Loss: 2.0170, Val Loss: 2.0171, F1 Score: 0.8004\n",
      "Fold 7 Epoch 98/100, Loss: 2.0161, Val Loss: 2.0171, F1 Score: 0.8004\n",
      "Fold 7 Epoch 99/100, Loss: 2.0159, Val Loss: 2.0171, F1 Score: 0.8003\n",
      "Fold 7 Epoch 100/100, Loss: 2.0160, Val Loss: 2.0171, F1 Score: 0.8004\n",
      "Starting Fold 8\n",
      "Fold 8 Epoch 1/100, Loss: 2.5231, Val Loss: 2.3160, F1 Score: 0.4115\n",
      "Fold 8 Epoch 2/100, Loss: 2.2678, Val Loss: 2.2484, F1 Score: 0.4441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8 Epoch 3/100, Loss: 2.2494, Val Loss: 2.2459, F1 Score: 0.4517\n",
      "Fold 8 Epoch 4/100, Loss: 2.2478, Val Loss: 2.2453, F1 Score: 0.4448\n",
      "Fold 8 Epoch 5/100, Loss: 2.2468, Val Loss: 2.2450, F1 Score: 0.4502\n",
      "Fold 8 Epoch 6/100, Loss: 2.2463, Val Loss: 2.2435, F1 Score: 0.4471\n",
      "Fold 8 Epoch 7/100, Loss: 2.2301, Val Loss: 2.1666, F1 Score: 0.5906\n",
      "Fold 8 Epoch 8/100, Loss: 2.1568, Val Loss: 2.1452, F1 Score: 0.5930\n",
      "Fold 8 Epoch 9/100, Loss: 2.1477, Val Loss: 2.1440, F1 Score: 0.5959\n",
      "Fold 8 Epoch 10/100, Loss: 2.1438, Val Loss: 2.1349, F1 Score: 0.5940\n",
      "Fold 8 Epoch 11/100, Loss: 2.1107, Val Loss: 2.0867, F1 Score: 0.6984\n",
      "Fold 8 Epoch 12/100, Loss: 2.0836, Val Loss: 2.0815, F1 Score: 0.6998\n",
      "Fold 8 Epoch 13/100, Loss: 2.0800, Val Loss: 2.0809, F1 Score: 0.6984\n",
      "Fold 8 Epoch 14/100, Loss: 2.0798, Val Loss: 2.0806, F1 Score: 0.6984\n",
      "Fold 8 Epoch 15/100, Loss: 2.0788, Val Loss: 2.0804, F1 Score: 0.6997\n",
      "Fold 8 Epoch 16/100, Loss: 2.0785, Val Loss: 2.0804, F1 Score: 0.7003\n",
      "Fold 8 Epoch 17/100, Loss: 2.0783, Val Loss: 2.0804, F1 Score: 0.7010\n",
      "Fold 8 Epoch 18/100, Loss: 2.0785, Val Loss: 2.0803, F1 Score: 0.7019\n",
      "Fold 8 Epoch 19/100, Loss: 2.0784, Val Loss: 2.0801, F1 Score: 0.6985\n",
      "Fold 8 Epoch 20/100, Loss: 2.0783, Val Loss: 2.0804, F1 Score: 0.7007\n",
      "Fold 8 Epoch 21/100, Loss: 2.0783, Val Loss: 2.0801, F1 Score: 0.7015\n",
      "Fold 8 Epoch 22/100, Loss: 2.0777, Val Loss: 2.0799, F1 Score: 0.7018\n",
      "Fold 8 Epoch 23/100, Loss: 2.0775, Val Loss: 2.0797, F1 Score: 0.7008\n",
      "Fold 8 Epoch 24/100, Loss: 2.0783, Val Loss: 2.0794, F1 Score: 0.7012\n",
      "Fold 8 Epoch 25/100, Loss: 2.0776, Val Loss: 2.0792, F1 Score: 0.6999\n",
      "Fold 8 Epoch 26/100, Loss: 2.0771, Val Loss: 2.0783, F1 Score: 0.7022\n",
      "Fold 8 Epoch 27/100, Loss: 2.0752, Val Loss: 2.0743, F1 Score: 0.7029\n",
      "Fold 8 Epoch 28/100, Loss: 2.0645, Val Loss: 2.0578, F1 Score: 0.7412\n",
      "Fold 8 Epoch 29/100, Loss: 2.0577, Val Loss: 2.0566, F1 Score: 0.7406\n",
      "Fold 8 Epoch 30/100, Loss: 2.0561, Val Loss: 2.0563, F1 Score: 0.7402\n",
      "Fold 8 Epoch 31/100, Loss: 2.0549, Val Loss: 2.0561, F1 Score: 0.7402\n",
      "Fold 8 Epoch 32/100, Loss: 2.0544, Val Loss: 2.0560, F1 Score: 0.7390\n",
      "Fold 8 Epoch 33/100, Loss: 2.0541, Val Loss: 2.0563, F1 Score: 0.7404\n",
      "Fold 8 Epoch 34/100, Loss: 2.0538, Val Loss: 2.0557, F1 Score: 0.7392\n",
      "Fold 8 Epoch 35/100, Loss: 2.0536, Val Loss: 2.0555, F1 Score: 0.7390\n",
      "Fold 8 Epoch 36/100, Loss: 2.0538, Val Loss: 2.0555, F1 Score: 0.7401\n",
      "Fold 8 Epoch 37/100, Loss: 2.0537, Val Loss: 2.0552, F1 Score: 0.7393\n",
      "Fold 8 Epoch 38/100, Loss: 2.0526, Val Loss: 2.0542, F1 Score: 0.7396\n",
      "Fold 8 Epoch 39/100, Loss: 2.0509, Val Loss: 2.0485, F1 Score: 0.7555\n",
      "Fold 8 Epoch 40/100, Loss: 2.0456, Val Loss: 2.0425, F1 Score: 0.7625\n",
      "Fold 8 Epoch 41/100, Loss: 2.0412, Val Loss: 2.0358, F1 Score: 0.7821\n",
      "Fold 8 Epoch 42/100, Loss: 2.0360, Val Loss: 2.0311, F1 Score: 0.7822\n",
      "Fold 8 Epoch 43/100, Loss: 2.0327, Val Loss: 2.0295, F1 Score: 0.7831\n",
      "Fold 8 Epoch 44/100, Loss: 2.0301, Val Loss: 2.0285, F1 Score: 0.7834\n",
      "Fold 8 Epoch 45/100, Loss: 2.0287, Val Loss: 2.0278, F1 Score: 0.7840\n",
      "Fold 8 Epoch 46/100, Loss: 2.0275, Val Loss: 2.0277, F1 Score: 0.7844\n",
      "Fold 8 Epoch 47/100, Loss: 2.0278, Val Loss: 2.0275, F1 Score: 0.7845\n",
      "Fold 8 Epoch 48/100, Loss: 2.0263, Val Loss: 2.0276, F1 Score: 0.7843\n",
      "Fold 8 Epoch 49/100, Loss: 2.0261, Val Loss: 2.0271, F1 Score: 0.7849\n",
      "Fold 8 Epoch 50/100, Loss: 2.0261, Val Loss: 2.0270, F1 Score: 0.7848\n",
      "Fold 8 Epoch 51/100, Loss: 2.0255, Val Loss: 2.0267, F1 Score: 0.7850\n",
      "Fold 8 Epoch 52/100, Loss: 2.0250, Val Loss: 2.0268, F1 Score: 0.7851\n",
      "Fold 8 Epoch 53/100, Loss: 2.0252, Val Loss: 2.0267, F1 Score: 0.7851\n",
      "Fold 8 Epoch 54/100, Loss: 2.0244, Val Loss: 2.0265, F1 Score: 0.7853\n",
      "Fold 8 Epoch 55/100, Loss: 2.0243, Val Loss: 2.0266, F1 Score: 0.7852\n",
      "Fold 8 Epoch 56/100, Loss: 2.0250, Val Loss: 2.0263, F1 Score: 0.7853\n",
      "Fold 8 Epoch 57/100, Loss: 2.0242, Val Loss: 2.0264, F1 Score: 0.7852\n",
      "Fold 8 Epoch 58/100, Loss: 2.0240, Val Loss: 2.0265, F1 Score: 0.7852\n",
      "Fold 8 Epoch 59/100, Loss: 2.0239, Val Loss: 2.0266, F1 Score: 0.7853\n",
      "Fold 8 Epoch 60/100, Loss: 2.0241, Val Loss: 2.0265, F1 Score: 0.7852\n",
      "Fold 8 Epoch 61/100, Loss: 2.0244, Val Loss: 2.0263, F1 Score: 0.7856\n",
      "Fold 8 Epoch 62/100, Loss: 2.0240, Val Loss: 2.0263, F1 Score: 0.7853\n",
      "Fold 8 Epoch 63/100, Loss: 2.0237, Val Loss: 2.0265, F1 Score: 0.7853\n",
      "Fold 8 Epoch 64/100, Loss: 2.0237, Val Loss: 2.0263, F1 Score: 0.7853\n",
      "Fold 8 Epoch 65/100, Loss: 2.0235, Val Loss: 2.0262, F1 Score: 0.7853\n",
      "Fold 8 Epoch 66/100, Loss: 2.0234, Val Loss: 2.0262, F1 Score: 0.7853\n",
      "Fold 8 Epoch 67/100, Loss: 2.0234, Val Loss: 2.0261, F1 Score: 0.7853\n",
      "Fold 8 Epoch 68/100, Loss: 2.0239, Val Loss: 2.0260, F1 Score: 0.7853\n",
      "Fold 8 Epoch 69/100, Loss: 2.0237, Val Loss: 2.0254, F1 Score: 0.7854\n",
      "Fold 8 Epoch 70/100, Loss: 2.0221, Val Loss: 2.0229, F1 Score: 0.7938\n",
      "Fold 8 Epoch 71/100, Loss: 2.0191, Val Loss: 2.0196, F1 Score: 0.7961\n",
      "Fold 8 Epoch 72/100, Loss: 2.0181, Val Loss: 2.0195, F1 Score: 0.7961\n",
      "Fold 8 Epoch 73/100, Loss: 2.0172, Val Loss: 2.0193, F1 Score: 0.7963\n",
      "Fold 8 Epoch 74/100, Loss: 2.0170, Val Loss: 2.0192, F1 Score: 0.7966\n",
      "Fold 8 Epoch 75/100, Loss: 2.0171, Val Loss: 2.0190, F1 Score: 0.7967\n",
      "Fold 8 Epoch 76/100, Loss: 2.0169, Val Loss: 2.0189, F1 Score: 0.7969\n",
      "Fold 8 Epoch 77/100, Loss: 2.0166, Val Loss: 2.0190, F1 Score: 0.7969\n",
      "Fold 8 Epoch 78/100, Loss: 2.0165, Val Loss: 2.0191, F1 Score: 0.7966\n",
      "Fold 8 Epoch 79/100, Loss: 2.0163, Val Loss: 2.0189, F1 Score: 0.7969\n",
      "Fold 8 Epoch 80/100, Loss: 2.0167, Val Loss: 2.0189, F1 Score: 0.7969\n",
      "Fold 8 Epoch 81/100, Loss: 2.0166, Val Loss: 2.0191, F1 Score: 0.7967\n",
      "Fold 8 Epoch 82/100, Loss: 2.0169, Val Loss: 2.0190, F1 Score: 0.7967\n",
      "Fold 8 Epoch 83/100, Loss: 2.0163, Val Loss: 2.0189, F1 Score: 0.7969\n",
      "Fold 8 Epoch 84/100, Loss: 2.0162, Val Loss: 2.0187, F1 Score: 0.7969\n",
      "Fold 8 Epoch 85/100, Loss: 2.0166, Val Loss: 2.0189, F1 Score: 0.7969\n",
      "Fold 8 Epoch 86/100, Loss: 2.0166, Val Loss: 2.0189, F1 Score: 0.7969\n",
      "Fold 8 Epoch 87/100, Loss: 2.0166, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 88/100, Loss: 2.0165, Val Loss: 2.0188, F1 Score: 0.7969\n",
      "Fold 8 Epoch 89/100, Loss: 2.0159, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 90/100, Loss: 2.0159, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 91/100, Loss: 2.0160, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 92/100, Loss: 2.0161, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 93/100, Loss: 2.0161, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 94/100, Loss: 2.0160, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 95/100, Loss: 2.0165, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 96/100, Loss: 2.0159, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 97/100, Loss: 2.0160, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 98/100, Loss: 2.0159, Val Loss: 2.0187, F1 Score: 0.7970\n",
      "Fold 8 Epoch 99/100, Loss: 2.0158, Val Loss: 2.0186, F1 Score: 0.7970\n",
      "Fold 8 Epoch 100/100, Loss: 2.0159, Val Loss: 2.0186, F1 Score: 0.7970\n",
      "Starting Fold 9\n",
      "Fold 9 Epoch 1/100, Loss: 2.5221, Val Loss: 2.3224, F1 Score: 0.4142\n",
      "Fold 9 Epoch 2/100, Loss: 2.2704, Val Loss: 2.2476, F1 Score: 0.4511\n",
      "Fold 9 Epoch 3/100, Loss: 2.2501, Val Loss: 2.2450, F1 Score: 0.4577\n",
      "Fold 9 Epoch 4/100, Loss: 2.2480, Val Loss: 2.2437, F1 Score: 0.4523\n",
      "Fold 9 Epoch 5/100, Loss: 2.2466, Val Loss: 2.2433, F1 Score: 0.4473\n",
      "Fold 9 Epoch 6/100, Loss: 2.2472, Val Loss: 2.2431, F1 Score: 0.4452\n",
      "Fold 9 Epoch 7/100, Loss: 2.2464, Val Loss: 2.2432, F1 Score: 0.4590\n",
      "Fold 9 Epoch 8/100, Loss: 2.2465, Val Loss: 2.2428, F1 Score: 0.4478\n",
      "Fold 9 Epoch 9/100, Loss: 2.2459, Val Loss: 2.2425, F1 Score: 0.4604\n",
      "Fold 9 Epoch 10/100, Loss: 2.2440, Val Loss: 2.2351, F1 Score: 0.4511\n",
      "Fold 9 Epoch 11/100, Loss: 2.1938, Val Loss: 2.1453, F1 Score: 0.5948\n",
      "Fold 9 Epoch 12/100, Loss: 2.1461, Val Loss: 2.1149, F1 Score: 0.6864\n",
      "Fold 9 Epoch 13/100, Loss: 2.0967, Val Loss: 2.0779, F1 Score: 0.7039\n",
      "Fold 9 Epoch 14/100, Loss: 2.0838, Val Loss: 2.0753, F1 Score: 0.7048\n",
      "Fold 9 Epoch 15/100, Loss: 2.0808, Val Loss: 2.0743, F1 Score: 0.7053\n",
      "Fold 9 Epoch 16/100, Loss: 2.0805, Val Loss: 2.0741, F1 Score: 0.7055\n",
      "Fold 9 Epoch 17/100, Loss: 2.0793, Val Loss: 2.0738, F1 Score: 0.7060\n",
      "Fold 9 Epoch 18/100, Loss: 2.0788, Val Loss: 2.0735, F1 Score: 0.7065\n",
      "Fold 9 Epoch 19/100, Loss: 2.0786, Val Loss: 2.0735, F1 Score: 0.7057\n",
      "Fold 9 Epoch 20/100, Loss: 2.0785, Val Loss: 2.0733, F1 Score: 0.7077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9 Epoch 21/100, Loss: 2.0786, Val Loss: 2.0731, F1 Score: 0.7094\n",
      "Fold 9 Epoch 22/100, Loss: 2.0779, Val Loss: 2.0726, F1 Score: 0.7093\n",
      "Fold 9 Epoch 23/100, Loss: 2.0778, Val Loss: 2.0716, F1 Score: 0.7100\n",
      "Fold 9 Epoch 24/100, Loss: 2.0758, Val Loss: 2.0676, F1 Score: 0.7102\n",
      "Fold 9 Epoch 25/100, Loss: 2.0666, Val Loss: 2.0492, F1 Score: 0.7510\n",
      "Fold 9 Epoch 26/100, Loss: 2.0584, Val Loss: 2.0478, F1 Score: 0.7489\n",
      "Fold 9 Epoch 27/100, Loss: 2.0566, Val Loss: 2.0474, F1 Score: 0.7495\n",
      "Fold 9 Epoch 28/100, Loss: 2.0559, Val Loss: 2.0471, F1 Score: 0.7500\n",
      "Fold 9 Epoch 29/100, Loss: 2.0557, Val Loss: 2.0469, F1 Score: 0.7501\n",
      "Fold 9 Epoch 30/100, Loss: 2.0546, Val Loss: 2.0464, F1 Score: 0.7499\n",
      "Fold 9 Epoch 31/100, Loss: 2.0538, Val Loss: 2.0442, F1 Score: 0.7491\n",
      "Fold 9 Epoch 32/100, Loss: 2.0496, Val Loss: 2.0355, F1 Score: 0.7728\n",
      "Fold 9 Epoch 33/100, Loss: 2.0454, Val Loss: 2.0337, F1 Score: 0.7729\n",
      "Fold 9 Epoch 34/100, Loss: 2.0436, Val Loss: 2.0332, F1 Score: 0.7725\n",
      "Fold 9 Epoch 35/100, Loss: 2.0416, Val Loss: 2.0272, F1 Score: 0.7836\n",
      "Fold 9 Epoch 36/100, Loss: 2.0366, Val Loss: 2.0212, F1 Score: 0.7945\n",
      "Fold 9 Epoch 37/100, Loss: 2.0322, Val Loss: 2.0193, F1 Score: 0.7942\n",
      "Fold 9 Epoch 38/100, Loss: 2.0296, Val Loss: 2.0179, F1 Score: 0.7950\n",
      "Fold 9 Epoch 39/100, Loss: 2.0285, Val Loss: 2.0175, F1 Score: 0.7956\n",
      "Fold 9 Epoch 40/100, Loss: 2.0276, Val Loss: 2.0175, F1 Score: 0.7953\n",
      "Fold 9 Epoch 41/100, Loss: 2.0271, Val Loss: 2.0171, F1 Score: 0.7953\n",
      "Fold 9 Epoch 42/100, Loss: 2.0263, Val Loss: 2.0170, F1 Score: 0.7955\n",
      "Fold 9 Epoch 43/100, Loss: 2.0260, Val Loss: 2.0170, F1 Score: 0.7956\n",
      "Fold 9 Epoch 44/100, Loss: 2.0258, Val Loss: 2.0171, F1 Score: 0.7953\n",
      "Fold 9 Epoch 45/100, Loss: 2.0256, Val Loss: 2.0170, F1 Score: 0.7955\n",
      "Fold 9 Epoch 46/100, Loss: 2.0261, Val Loss: 2.0170, F1 Score: 0.7956\n",
      "Fold 9 Epoch 47/100, Loss: 2.0254, Val Loss: 2.0169, F1 Score: 0.7957\n",
      "Fold 9 Epoch 48/100, Loss: 2.0253, Val Loss: 2.0169, F1 Score: 0.7957\n",
      "Fold 9 Epoch 49/100, Loss: 2.0257, Val Loss: 2.0170, F1 Score: 0.7957\n",
      "Fold 9 Epoch 50/100, Loss: 2.0253, Val Loss: 2.0169, F1 Score: 0.7960\n",
      "Fold 9 Epoch 51/100, Loss: 2.0250, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 52/100, Loss: 2.0249, Val Loss: 2.0168, F1 Score: 0.7959\n",
      "Fold 9 Epoch 53/100, Loss: 2.0253, Val Loss: 2.0169, F1 Score: 0.7961\n",
      "Fold 9 Epoch 54/100, Loss: 2.0253, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 55/100, Loss: 2.0247, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 56/100, Loss: 2.0245, Val Loss: 2.0167, F1 Score: 0.7957\n",
      "Fold 9 Epoch 57/100, Loss: 2.0246, Val Loss: 2.0167, F1 Score: 0.7957\n",
      "Fold 9 Epoch 58/100, Loss: 2.0245, Val Loss: 2.0167, F1 Score: 0.7957\n",
      "Fold 9 Epoch 59/100, Loss: 2.0254, Val Loss: 2.0168, F1 Score: 0.7958\n",
      "Fold 9 Epoch 60/100, Loss: 2.0244, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 61/100, Loss: 2.0244, Val Loss: 2.0167, F1 Score: 0.7959\n",
      "Fold 9 Epoch 62/100, Loss: 2.0243, Val Loss: 2.0167, F1 Score: 0.7956\n",
      "Fold 9 Epoch 63/100, Loss: 2.0247, Val Loss: 2.0167, F1 Score: 0.7956\n",
      "Fold 9 Epoch 64/100, Loss: 2.0247, Val Loss: 2.0167, F1 Score: 0.7957\n",
      "Fold 9 Epoch 65/100, Loss: 2.0248, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 66/100, Loss: 2.0242, Val Loss: 2.0167, F1 Score: 0.7957\n",
      "Fold 9 Epoch 67/100, Loss: 2.0242, Val Loss: 2.0167, F1 Score: 0.7957\n",
      "Fold 9 Epoch 68/100, Loss: 2.0247, Val Loss: 2.0167, F1 Score: 0.7957\n",
      "Fold 9 Epoch 69/100, Loss: 2.0242, Val Loss: 2.0167, F1 Score: 0.7957\n",
      "Fold 9 Epoch 70/100, Loss: 2.0247, Val Loss: 2.0167, F1 Score: 0.7959\n",
      "Fold 9 Epoch 71/100, Loss: 2.0241, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 72/100, Loss: 2.0241, Val Loss: 2.0168, F1 Score: 0.7959\n",
      "Fold 9 Epoch 73/100, Loss: 2.0246, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 74/100, Loss: 2.0241, Val Loss: 2.0167, F1 Score: 0.7959\n",
      "Fold 9 Epoch 75/100, Loss: 2.0246, Val Loss: 2.0167, F1 Score: 0.7959\n",
      "Fold 9 Epoch 76/100, Loss: 2.0246, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 77/100, Loss: 2.0241, Val Loss: 2.0167, F1 Score: 0.7957\n",
      "Fold 9 Epoch 78/100, Loss: 2.0241, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 79/100, Loss: 2.0246, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 80/100, Loss: 2.0240, Val Loss: 2.0167, F1 Score: 0.7958\n",
      "Fold 9 Epoch 81/100, Loss: 2.0241, Val Loss: 2.0166, F1 Score: 0.7958\n",
      "Fold 9 Epoch 82/100, Loss: 2.0240, Val Loss: 2.0167, F1 Score: 0.7959\n",
      "Fold 9 Epoch 83/100, Loss: 2.0240, Val Loss: 2.0167, F1 Score: 0.7959\n",
      "Fold 9 Epoch 84/100, Loss: 2.0240, Val Loss: 2.0167, F1 Score: 0.7960\n",
      "Fold 9 Epoch 85/100, Loss: 2.0241, Val Loss: 2.0168, F1 Score: 0.7960\n",
      "Fold 9 Epoch 86/100, Loss: 2.0240, Val Loss: 2.0166, F1 Score: 0.7958\n",
      "Fold 9 Epoch 87/100, Loss: 2.0245, Val Loss: 2.0167, F1 Score: 0.7959\n",
      "Fold 9 Epoch 88/100, Loss: 2.0245, Val Loss: 2.0167, F1 Score: 0.7959\n",
      "Fold 9 Epoch 89/100, Loss: 2.0240, Val Loss: 2.0166, F1 Score: 0.7959\n",
      "Fold 9 Epoch 90/100, Loss: 2.0239, Val Loss: 2.0166, F1 Score: 0.7957\n",
      "Fold 9 Epoch 91/100, Loss: 2.0239, Val Loss: 2.0166, F1 Score: 0.7959\n",
      "Fold 9 Epoch 92/100, Loss: 2.0240, Val Loss: 2.0165, F1 Score: 0.7958\n",
      "Fold 9 Epoch 93/100, Loss: 2.0239, Val Loss: 2.0165, F1 Score: 0.7958\n",
      "Fold 9 Epoch 94/100, Loss: 2.0244, Val Loss: 2.0165, F1 Score: 0.7957\n",
      "Fold 9 Epoch 95/100, Loss: 2.0239, Val Loss: 2.0164, F1 Score: 0.7960\n",
      "Fold 9 Epoch 96/100, Loss: 2.0236, Val Loss: 2.0160, F1 Score: 0.7956\n",
      "Fold 9 Epoch 97/100, Loss: 2.0235, Val Loss: 2.0150, F1 Score: 0.7973\n",
      "Fold 9 Epoch 98/100, Loss: 2.0223, Val Loss: 2.0129, F1 Score: 0.8051\n",
      "Fold 9 Epoch 99/100, Loss: 2.0202, Val Loss: 2.0110, F1 Score: 0.8061\n",
      "Fold 9 Epoch 100/100, Loss: 2.0197, Val Loss: 2.0108, F1 Score: 0.8061\n",
      "Starting Fold 10\n",
      "Fold 10 Epoch 1/100, Loss: 2.5402, Val Loss: 2.3374, F1 Score: 0.4127\n",
      "Fold 10 Epoch 2/100, Loss: 2.2754, Val Loss: 2.2453, F1 Score: 0.4589\n",
      "Fold 10 Epoch 3/100, Loss: 2.2503, Val Loss: 2.2417, F1 Score: 0.4528\n",
      "Fold 10 Epoch 4/100, Loss: 2.2478, Val Loss: 2.2419, F1 Score: 0.4591\n",
      "Fold 10 Epoch 5/100, Loss: 2.2470, Val Loss: 2.2411, F1 Score: 0.4518\n",
      "Fold 10 Epoch 6/100, Loss: 2.2473, Val Loss: 2.2406, F1 Score: 0.4468\n",
      "Fold 10 Epoch 7/100, Loss: 2.2459, Val Loss: 2.2404, F1 Score: 0.4532\n",
      "Fold 10 Epoch 8/100, Loss: 2.2454, Val Loss: 2.2389, F1 Score: 0.4589\n",
      "Fold 10 Epoch 9/100, Loss: 2.2301, Val Loss: 2.1656, F1 Score: 0.5963\n",
      "Fold 10 Epoch 10/100, Loss: 2.1565, Val Loss: 2.1411, F1 Score: 0.5960\n",
      "Fold 10 Epoch 11/100, Loss: 2.1432, Val Loss: 2.1180, F1 Score: 0.6848\n",
      "Fold 10 Epoch 12/100, Loss: 2.1021, Val Loss: 2.0820, F1 Score: 0.7039\n",
      "Fold 10 Epoch 13/100, Loss: 2.0872, Val Loss: 2.0780, F1 Score: 0.7052\n",
      "Fold 10 Epoch 14/100, Loss: 2.0833, Val Loss: 2.0778, F1 Score: 0.7016\n",
      "Fold 10 Epoch 15/100, Loss: 2.0826, Val Loss: 2.0775, F1 Score: 0.7046\n",
      "Fold 10 Epoch 16/100, Loss: 2.0813, Val Loss: 2.0771, F1 Score: 0.7028\n",
      "Fold 10 Epoch 17/100, Loss: 2.0800, Val Loss: 2.0765, F1 Score: 0.7018\n",
      "Fold 10 Epoch 18/100, Loss: 2.0798, Val Loss: 2.0766, F1 Score: 0.7020\n",
      "Fold 10 Epoch 19/100, Loss: 2.0792, Val Loss: 2.0762, F1 Score: 0.7017\n",
      "Fold 10 Epoch 20/100, Loss: 2.0791, Val Loss: 2.0758, F1 Score: 0.7017\n",
      "Fold 10 Epoch 21/100, Loss: 2.0786, Val Loss: 2.0752, F1 Score: 0.7019\n",
      "Fold 10 Epoch 22/100, Loss: 2.0780, Val Loss: 2.0747, F1 Score: 0.7028\n",
      "Fold 10 Epoch 23/100, Loss: 2.0774, Val Loss: 2.0738, F1 Score: 0.7036\n",
      "Fold 10 Epoch 24/100, Loss: 2.0766, Val Loss: 2.0713, F1 Score: 0.7028\n",
      "Fold 10 Epoch 25/100, Loss: 2.0703, Val Loss: 2.0575, F1 Score: 0.7458\n",
      "Fold 10 Epoch 26/100, Loss: 2.0595, Val Loss: 2.0539, F1 Score: 0.7454\n",
      "Fold 10 Epoch 27/100, Loss: 2.0572, Val Loss: 2.0533, F1 Score: 0.7454\n",
      "Fold 10 Epoch 28/100, Loss: 2.0561, Val Loss: 2.0531, F1 Score: 0.7444\n",
      "Fold 10 Epoch 29/100, Loss: 2.0552, Val Loss: 2.0528, F1 Score: 0.7452\n",
      "Fold 10 Epoch 30/100, Loss: 2.0551, Val Loss: 2.0527, F1 Score: 0.7451\n",
      "Fold 10 Epoch 31/100, Loss: 2.0552, Val Loss: 2.0527, F1 Score: 0.7460\n",
      "Fold 10 Epoch 32/100, Loss: 2.0544, Val Loss: 2.0525, F1 Score: 0.7454\n",
      "Fold 10 Epoch 33/100, Loss: 2.0548, Val Loss: 2.0523, F1 Score: 0.7448\n",
      "Fold 10 Epoch 34/100, Loss: 2.0541, Val Loss: 2.0524, F1 Score: 0.7450\n",
      "Fold 10 Epoch 35/100, Loss: 2.0539, Val Loss: 2.0521, F1 Score: 0.7443\n",
      "Fold 10 Epoch 36/100, Loss: 2.0543, Val Loss: 2.0520, F1 Score: 0.7447\n",
      "Fold 10 Epoch 37/100, Loss: 2.0538, Val Loss: 2.0520, F1 Score: 0.7440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 Epoch 38/100, Loss: 2.0535, Val Loss: 2.0519, F1 Score: 0.7435\n",
      "Fold 10 Epoch 39/100, Loss: 2.0540, Val Loss: 2.0520, F1 Score: 0.7433\n",
      "Fold 10 Epoch 40/100, Loss: 2.0534, Val Loss: 2.0518, F1 Score: 0.7438\n",
      "Fold 10 Epoch 41/100, Loss: 2.0539, Val Loss: 2.0519, F1 Score: 0.7436\n",
      "Fold 10 Epoch 42/100, Loss: 2.0534, Val Loss: 2.0519, F1 Score: 0.7448\n",
      "Fold 10 Epoch 43/100, Loss: 2.0533, Val Loss: 2.0517, F1 Score: 0.7434\n",
      "Fold 10 Epoch 44/100, Loss: 2.0532, Val Loss: 2.0517, F1 Score: 0.7436\n",
      "Fold 10 Epoch 45/100, Loss: 2.0531, Val Loss: 2.0516, F1 Score: 0.7436\n",
      "Fold 10 Epoch 46/100, Loss: 2.0531, Val Loss: 2.0516, F1 Score: 0.7444\n",
      "Fold 10 Epoch 47/100, Loss: 2.0533, Val Loss: 2.0507, F1 Score: 0.7446\n",
      "Fold 10 Epoch 48/100, Loss: 2.0486, Val Loss: 2.0395, F1 Score: 0.7665\n",
      "Fold 10 Epoch 49/100, Loss: 2.0433, Val Loss: 2.0372, F1 Score: 0.7674\n",
      "Fold 10 Epoch 50/100, Loss: 2.0412, Val Loss: 2.0365, F1 Score: 0.7679\n",
      "Fold 10 Epoch 51/100, Loss: 2.0396, Val Loss: 2.0346, F1 Score: 0.7675\n",
      "Fold 10 Epoch 52/100, Loss: 2.0340, Val Loss: 2.0249, F1 Score: 0.7883\n",
      "Fold 10 Epoch 53/100, Loss: 2.0306, Val Loss: 2.0253, F1 Score: 0.7872\n",
      "Fold 10 Epoch 54/100, Loss: 2.0283, Val Loss: 2.0242, F1 Score: 0.7883\n",
      "Fold 10 Epoch 55/100, Loss: 2.0280, Val Loss: 2.0248, F1 Score: 0.7876\n",
      "Fold 10 Epoch 56/100, Loss: 2.0272, Val Loss: 2.0231, F1 Score: 0.7892\n",
      "Fold 10 Epoch 57/100, Loss: 2.0261, Val Loss: 2.0230, F1 Score: 0.7890\n",
      "Fold 10 Epoch 58/100, Loss: 2.0259, Val Loss: 2.0231, F1 Score: 0.7888\n",
      "Fold 10 Epoch 59/100, Loss: 2.0257, Val Loss: 2.0230, F1 Score: 0.7887\n",
      "Fold 10 Epoch 60/100, Loss: 2.0256, Val Loss: 2.0227, F1 Score: 0.7890\n",
      "Fold 10 Epoch 61/100, Loss: 2.0249, Val Loss: 2.0231, F1 Score: 0.7886\n",
      "Fold 10 Epoch 62/100, Loss: 2.0256, Val Loss: 2.0229, F1 Score: 0.7890\n",
      "Fold 10 Epoch 63/100, Loss: 2.0249, Val Loss: 2.0227, F1 Score: 0.7890\n",
      "Fold 10 Epoch 64/100, Loss: 2.0248, Val Loss: 2.0229, F1 Score: 0.7886\n",
      "Fold 10 Epoch 65/100, Loss: 2.0244, Val Loss: 2.0227, F1 Score: 0.7888\n",
      "Fold 10 Epoch 66/100, Loss: 2.0245, Val Loss: 2.0227, F1 Score: 0.7889\n",
      "Fold 10 Epoch 67/100, Loss: 2.0244, Val Loss: 2.0227, F1 Score: 0.7887\n",
      "Fold 10 Epoch 68/100, Loss: 2.0244, Val Loss: 2.0227, F1 Score: 0.7886\n",
      "Fold 10 Epoch 69/100, Loss: 2.0243, Val Loss: 2.0226, F1 Score: 0.7889\n",
      "Fold 10 Epoch 70/100, Loss: 2.0241, Val Loss: 2.0226, F1 Score: 0.7889\n",
      "Fold 10 Epoch 71/100, Loss: 2.0244, Val Loss: 2.0228, F1 Score: 0.7888\n",
      "Fold 10 Epoch 72/100, Loss: 2.0247, Val Loss: 2.0226, F1 Score: 0.7892\n",
      "Fold 10 Epoch 73/100, Loss: 2.0243, Val Loss: 2.0228, F1 Score: 0.7890\n",
      "Fold 10 Epoch 74/100, Loss: 2.0243, Val Loss: 2.0225, F1 Score: 0.7890\n",
      "Fold 10 Epoch 75/100, Loss: 2.0241, Val Loss: 2.0227, F1 Score: 0.7889\n",
      "Fold 10 Epoch 76/100, Loss: 2.0242, Val Loss: 2.0227, F1 Score: 0.7889\n",
      "Fold 10 Epoch 77/100, Loss: 2.0239, Val Loss: 2.0225, F1 Score: 0.7891\n",
      "Fold 10 Epoch 78/100, Loss: 2.0240, Val Loss: 2.0225, F1 Score: 0.7890\n",
      "Fold 10 Epoch 79/100, Loss: 2.0240, Val Loss: 2.0226, F1 Score: 0.7889\n",
      "Fold 10 Epoch 80/100, Loss: 2.0243, Val Loss: 2.0226, F1 Score: 0.7888\n",
      "Fold 10 Epoch 81/100, Loss: 2.0251, Val Loss: 2.0225, F1 Score: 0.7889\n",
      "Fold 10 Epoch 82/100, Loss: 2.0245, Val Loss: 2.0225, F1 Score: 0.7887\n",
      "Fold 10 Epoch 83/100, Loss: 2.0245, Val Loss: 2.0226, F1 Score: 0.7887\n",
      "Fold 10 Epoch 84/100, Loss: 2.0238, Val Loss: 2.0226, F1 Score: 0.7888\n",
      "Fold 10 Epoch 85/100, Loss: 2.0238, Val Loss: 2.0225, F1 Score: 0.7890\n",
      "Fold 10 Epoch 86/100, Loss: 2.0242, Val Loss: 2.0225, F1 Score: 0.7890\n",
      "Fold 10 Epoch 87/100, Loss: 2.0243, Val Loss: 2.0225, F1 Score: 0.7890\n",
      "Fold 10 Epoch 88/100, Loss: 2.0248, Val Loss: 2.0225, F1 Score: 0.7888\n",
      "Fold 10 Epoch 89/100, Loss: 2.0242, Val Loss: 2.0225, F1 Score: 0.7891\n",
      "Fold 10 Epoch 90/100, Loss: 2.0238, Val Loss: 2.0225, F1 Score: 0.7891\n",
      "Fold 10 Epoch 91/100, Loss: 2.0237, Val Loss: 2.0225, F1 Score: 0.7889\n",
      "Fold 10 Epoch 92/100, Loss: 2.0236, Val Loss: 2.0225, F1 Score: 0.7889\n",
      "Fold 10 Epoch 93/100, Loss: 2.0237, Val Loss: 2.0225, F1 Score: 0.7889\n",
      "Fold 10 Epoch 94/100, Loss: 2.0237, Val Loss: 2.0227, F1 Score: 0.7888\n",
      "Fold 10 Epoch 95/100, Loss: 2.0242, Val Loss: 2.0226, F1 Score: 0.7888\n",
      "Fold 10 Epoch 96/100, Loss: 2.0236, Val Loss: 2.0226, F1 Score: 0.7890\n",
      "Fold 10 Epoch 97/100, Loss: 2.0242, Val Loss: 2.0225, F1 Score: 0.7890\n",
      "Fold 10 Epoch 98/100, Loss: 2.0237, Val Loss: 2.0225, F1 Score: 0.7887\n",
      "Fold 10 Epoch 99/100, Loss: 2.0236, Val Loss: 2.0225, F1 Score: 0.7891\n",
      "Fold 10 Epoch 100/100, Loss: 2.0242, Val Loss: 2.0225, F1 Score: 0.7889\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "best_model = None\n",
    "best_f1_score = 0  # Track the best F1 score\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train_tensor, y_train_tensor)):\n",
    "    print(f\"Starting Fold {fold + 1}\")\n",
    "    \n",
    "    # Split data into train and validation sets for this fold\n",
    "    X_fold_train, X_fold_val = X_train_tensor[train_index], X_train_tensor[val_index]\n",
    "    y_fold_train, y_fold_val = y_train_tensor[train_index], y_train_tensor[val_index]\n",
    "    \n",
    "    # Move data to the specified device\n",
    "    X_fold_train = X_fold_train.to(device)\n",
    "    X_fold_val = X_fold_val.to(device)\n",
    "    y_fold_train = y_fold_train.to(device)\n",
    "    y_fold_val = y_fold_val.to(device)\n",
    "    \n",
    "    # Create DataLoaders for this fold\n",
    "    train_dataset = TensorDataset(X_fold_train, y_fold_train)\n",
    "    val_dataset = TensorDataset(X_fold_val, y_fold_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    \n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = DeepARGMLP(input_dim=X_train.shape[1], output_dim=num_classes).to(device)  # Move model to device\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    # Train the model for each fold with 100 epochs\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move batch to device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluate on the fold validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)  # Move batch to device\n",
    "                outputs = model(X_val_batch)\n",
    "                loss = criterion(outputs, y_val_batch)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())  # Move to CPU for metric calculation\n",
    "                all_labels.extend(y_val_batch.cpu().numpy())\n",
    "        \n",
    "        # Calculate F1 score for this epoch\n",
    "        fold_f1_score = f1_score(all_labels, all_predictions, average='weighted')\n",
    "        print(f\"Fold {fold + 1} Epoch {epoch + 1}/{epochs}, \"\n",
    "              f\"Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "              f\"F1 Score: {fold_f1_score:.4f}\")\n",
    "    \n",
    "    # Save the best model based on F1 score\n",
    "    if fold_f1_score > best_f1_score:\n",
    "        best_f1_score = fold_f1_score\n",
    "        best_model = model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8014970490859363\n",
      "Recall: 0.8014970490859363\n",
      "F1 Score: 0.8014970490859363\n",
      "Macro Precision: 0.5427454283262227\n",
      "Macro Recall: 0.43059994325143236\n",
      "Macro F1 Score: 0.468465276830728\n",
      "Weighted Precision: 0.8735935060920231\n",
      "Weighted Recall: 0.8014970490859363\n",
      "Weighted F1 Score: 0.8011653461489525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_val_batch, y_val_batch in val_loader:\n",
    "        outputs = best_model(X_val_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_val_batch.cpu().numpy())\n",
    "\n",
    "# Calculate macro precision, recall, and F1 score, as well as per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels, all_predictions, average=None, labels=range(num_classes)\n",
    ")\n",
    "\n",
    "avg_precision, avg_recall, avg_f1, avg_support = precision_recall_fscore_support(\n",
    "    all_labels, all_predictions, average='micro'\n",
    ")\n",
    "\n",
    "# Calculate macro-averaged metrics (ignoring class imbalance)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "    all_labels, all_predictions, average='macro'\n",
    ")\n",
    "\n",
    "weighted_precision, weighted_recall, weighted_f1, weighted_support = precision_recall_fscore_support(\n",
    "    all_labels, all_predictions, average='weighted'\n",
    ")\n",
    "\n",
    "# Print macro-averaged metrics\n",
    "print(f\"Precision: {avg_precision}\")\n",
    "print(f\"Recall: {avg_recall}\")\n",
    "print(f\"F1 Score: {avg_f1}\")\n",
    "\n",
    "print(f\"Macro Precision: {macro_precision}\")\n",
    "print(f\"Macro Recall: {macro_recall}\")\n",
    "print(f\"Macro F1 Score: {macro_f1}\")\n",
    "\n",
    "print(f\"Weighted Precision: {weighted_precision}\")\n",
    "print(f\"Weighted Recall: {weighted_recall}\")\n",
    "print(f\"Weighted F1 Score: {weighted_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'aminoglycoside': Precision: 0.9941860465116279, Recall: 0.684, F1 Score: 0.8104265402843601, Support: 250\n",
      "Class 'bacitracin': Precision: 0.6160290421669925, Recall: 1.0, F1 Score: 0.7623984793502678, Support: 2206\n",
      "Class 'beta_lactam': Precision: 1.0, Recall: 0.785109228711547, F1 Score: 0.8796203796203796, Support: 2243\n",
      "Class 'chloramphenicol': Precision: 1.0, Recall: 0.6289308176100629, F1 Score: 0.7722007722007722, Support: 159\n",
      "Class 'fosfomycin': Precision: 1.0, Recall: 0.8333333333333334, F1 Score: 0.9090909090909091, Support: 60\n",
      "Class 'fosmidomycin': Precision: 0.0, Recall: 0.0, F1 Score: 0.0, Support: 1\n",
      "Class 'glycopeptide': Precision: 0.0, Recall: 0.0, F1 Score: 0.0, Support: 13\n",
      "Class 'macrolide-lincosamide-streptogramin': Precision: 1.0, Recall: 0.4573170731707317, F1 Score: 0.6276150627615064, Support: 984\n",
      "Class 'multidrug': Precision: 0.990990990990991, Recall: 0.8270676691729323, F1 Score: 0.901639344262295, Support: 133\n",
      "Class 'mupirocin': Precision: 0.0, Recall: 0.0, F1 Score: 0.0, Support: 8\n",
      "Class 'polymyxin': Precision: 0.997229916897507, Recall: 0.8126410835214447, F1 Score: 0.8955223880597015, Support: 886\n",
      "Class 'quinolone': Precision: 0.0, Recall: 0.0, F1 Score: 0.0, Support: 1\n",
      "Class 'sulfonamide': Precision: 0.0, Recall: 0.0, F1 Score: 0.0, Support: 1\n",
      "Class 'tetracycline': Precision: 0.0, Recall: 0.0, F1 Score: 0.0, Support: 2\n",
      "Class 'trimethoprim': Precision: 0.0, Recall: 0.0, F1 Score: 0.0, Support: 0\n"
     ]
    }
   ],
   "source": [
    "class_names = label_encoder.classes_\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"Class '{class_name}': Precision: {precision[i]}, Recall: {recall[i]}, F1 Score: {f1[i]}, Support: {support[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/best_sr_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/best_sr_model.pth\"\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(best_model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-f1adbb4d2d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX_test_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mall_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ace246d3b150>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "output_path = \"results/sr_model_predictions.csv\"\n",
    "\n",
    "# After evaluating the best model on the holdout set and collecting predictions\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_test_batch, y_test_batch in test_loader:\n",
    "        outputs = best_model(X_test_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_test_batch.cpu().numpy())\n",
    "\n",
    "# Decode labels and predictions to their original class names\n",
    "true_labels = label_encoder.inverse_transform(all_labels)\n",
    "predicted_labels = label_encoder.inverse_transform(all_predictions)\n",
    "\n",
    "# Include the ID (index) from y_val\n",
    "ids = range(len(y_val))  # Assuming y_val is the original holdout labels array\n",
    "print(len(y_val))\n",
    "print(len(all_labels))\n",
    "print(len(all_predictions))\n",
    "\n",
    "# Create a DataFrame to store the outputs\n",
    "outputs_df = pd.DataFrame({\n",
    "    \"ID\": ids,\n",
    "    \"True Label\": true_labels,\n",
    "    \"Predicted Label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save the outputs to a CSV file\n",
    "outputs_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f3e60312150>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3)",
   "language": "python",
   "name": "conda_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
